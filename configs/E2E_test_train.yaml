data:
  mode: "local_json"
  train_path: "data/dummy_chat_data.jsonl"

model:
  tokenizer: "Qwen/Qwen3-0.6B"
  max_seq_len: 256
  hidden_size: 512
  num_hidden_layers: 3
  bias: False
  max_attention_experts: 32
  min_attention_experts: 8
  head_dim: 32
  max_moe_experts: 32
  min_moe_experts: 8
  intermediate_size: 32
  k_reborn_experts: -1
  w_ce_smha: 1.0
  w_kl_smha: 1.0
  w_aux_smha: 1.0
  w_ce_moe: 1.0
  w_kl_moe: 1.0
  w_aux_moe: 1.0
  pi_alpha: 32
  pi_gamma: 0.5

train:
  learning_rate: 1e-4
  epochs: 30
  batch_size: 1
  log_interval: 10

observer:
  output_dir: "output/E2E_test_train"
