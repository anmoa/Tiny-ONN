{
    "0": "lm_head",
    "1": "model",
    "2": "model.embed_tokens",
    "3": "model.layers",
    "4": "model.layers.0",
    "5": "model.layers.0.input_layernorm",
    "6": "model.layers.0.mlp",
    "7": "model.layers.0.mlp.act_fn",
    "8": "model.layers.0.mlp.down_proj",
    "9": "model.layers.0.mlp.gate_proj",
    "10": "model.layers.0.mlp.up_proj",
    "11": "model.layers.0.post_attention_layernorm",
    "12": "model.layers.0.self_attn",
    "13": "model.layers.0.self_attn.k_norm",
    "14": "model.layers.0.self_attn.k_proj",
    "15": "model.layers.0.self_attn.o_proj",
    "16": "model.layers.0.self_attn.q_norm",
    "17": "model.layers.0.self_attn.q_proj",
    "18": "model.layers.0.self_attn.v_proj",
    "19": "model.layers.1",
    "20": "model.layers.1.input_layernorm",
    "21": "model.layers.1.mlp",
    "22": "model.layers.1.mlp.act_fn",
    "23": "model.layers.1.mlp.down_proj",
    "24": "model.layers.1.mlp.gate_proj",
    "25": "model.layers.1.mlp.up_proj",
    "26": "model.layers.1.post_attention_layernorm",
    "27": "model.layers.1.self_attn",
    "28": "model.layers.1.self_attn.k_norm",
    "29": "model.layers.1.self_attn.k_proj",
    "30": "model.layers.1.self_attn.o_proj",
    "31": "model.layers.1.self_attn.q_norm",
    "32": "model.layers.1.self_attn.q_proj",
    "33": "model.layers.1.self_attn.v_proj",
    "34": "model.layers.10",
    "35": "model.layers.10.input_layernorm",
    "36": "model.layers.10.mlp",
    "37": "model.layers.10.mlp.act_fn",
    "38": "model.layers.10.mlp.down_proj",
    "39": "model.layers.10.mlp.gate_proj",
    "40": "model.layers.10.mlp.up_proj",
    "41": "model.layers.10.post_attention_layernorm",
    "42": "model.layers.10.self_attn",
    "43": "model.layers.10.self_attn.k_norm",
    "44": "model.layers.10.self_attn.k_proj",
    "45": "model.layers.10.self_attn.o_proj",
    "46": "model.layers.10.self_attn.q_norm",
    "47": "model.layers.10.self_attn.q_proj",
    "48": "model.layers.10.self_attn.v_proj",
    "49": "model.layers.11",
    "50": "model.layers.11.input_layernorm",
    "51": "model.layers.11.mlp",
    "52": "model.layers.11.mlp.act_fn",
    "53": "model.layers.11.mlp.down_proj",
    "54": "model.layers.11.mlp.gate_proj",
    "55": "model.layers.11.mlp.up_proj",
    "56": "model.layers.11.post_attention_layernorm",
    "57": "model.layers.11.self_attn",
    "58": "model.layers.11.self_attn.k_norm",
    "59": "model.layers.11.self_attn.k_proj",
    "60": "model.layers.11.self_attn.o_proj",
    "61": "model.layers.11.self_attn.q_norm",
    "62": "model.layers.11.self_attn.q_proj",
    "63": "model.layers.11.self_attn.v_proj",
    "64": "model.layers.12",
    "65": "model.layers.12.input_layernorm",
    "66": "model.layers.12.mlp",
    "67": "model.layers.12.mlp.act_fn",
    "68": "model.layers.12.mlp.down_proj",
    "69": "model.layers.12.mlp.gate_proj",
    "70": "model.layers.12.mlp.up_proj",
    "71": "model.layers.12.post_attention_layernorm",
    "72": "model.layers.12.self_attn",
    "73": "model.layers.12.self_attn.k_norm",
    "74": "model.layers.12.self_attn.k_proj",
    "75": "model.layers.12.self_attn.o_proj",
    "76": "model.layers.12.self_attn.q_norm",
    "77": "model.layers.12.self_attn.q_proj",
    "78": "model.layers.12.self_attn.v_proj",
    "79": "model.layers.13",
    "80": "model.layers.13.input_layernorm",
    "81": "model.layers.13.mlp",
    "82": "model.layers.13.mlp.act_fn",
    "83": "model.layers.13.mlp.down_proj",
    "84": "model.layers.13.mlp.gate_proj",
    "85": "model.layers.13.mlp.up_proj",
    "86": "model.layers.13.post_attention_layernorm",
    "87": "model.layers.13.self_attn",
    "88": "model.layers.13.self_attn.k_norm",
    "89": "model.layers.13.self_attn.k_proj",
    "90": "model.layers.13.self_attn.o_proj",
    "91": "model.layers.13.self_attn.q_norm",
    "92": "model.layers.13.self_attn.q_proj",
    "93": "model.layers.13.self_attn.v_proj",
    "94": "model.layers.14",
    "95": "model.layers.14.input_layernorm",
    "96": "model.layers.14.mlp",
    "97": "model.layers.14.mlp.act_fn",
    "98": "model.layers.14.mlp.down_proj",
    "99": "model.layers.14.mlp.gate_proj",
    "100": "model.layers.14.mlp.up_proj",
    "101": "model.layers.14.post_attention_layernorm",
    "102": "model.layers.14.self_attn",
    "103": "model.layers.14.self_attn.k_norm",
    "104": "model.layers.14.self_attn.k_proj",
    "105": "model.layers.14.self_attn.o_proj",
    "106": "model.layers.14.self_attn.q_norm",
    "107": "model.layers.14.self_attn.q_proj",
    "108": "model.layers.14.self_attn.v_proj",
    "109": "model.layers.15",
    "110": "model.layers.15.input_layernorm",
    "111": "model.layers.15.mlp",
    "112": "model.layers.15.mlp.act_fn",
    "113": "model.layers.15.mlp.down_proj",
    "114": "model.layers.15.mlp.gate_proj",
    "115": "model.layers.15.mlp.up_proj",
    "116": "model.layers.15.post_attention_layernorm",
    "117": "model.layers.15.self_attn",
    "118": "model.layers.15.self_attn.k_norm",
    "119": "model.layers.15.self_attn.k_proj",
    "120": "model.layers.15.self_attn.o_proj",
    "121": "model.layers.15.self_attn.q_norm",
    "122": "model.layers.15.self_attn.q_proj",
    "123": "model.layers.15.self_attn.v_proj",
    "124": "model.layers.16",
    "125": "model.layers.16.input_layernorm",
    "126": "model.layers.16.mlp",
    "127": "model.layers.16.mlp.act_fn",
    "128": "model.layers.16.mlp.down_proj",
    "129": "model.layers.16.mlp.gate_proj",
    "130": "model.layers.16.mlp.up_proj",
    "131": "model.layers.16.post_attention_layernorm",
    "132": "model.layers.16.self_attn",
    "133": "model.layers.16.self_attn.k_norm",
    "134": "model.layers.16.self_attn.k_proj",
    "135": "model.layers.16.self_attn.o_proj",
    "136": "model.layers.16.self_attn.q_norm",
    "137": "model.layers.16.self_attn.q_proj",
    "138": "model.layers.16.self_attn.v_proj",
    "139": "model.layers.17",
    "140": "model.layers.17.input_layernorm",
    "141": "model.layers.17.mlp",
    "142": "model.layers.17.mlp.act_fn",
    "143": "model.layers.17.mlp.down_proj",
    "144": "model.layers.17.mlp.gate_proj",
    "145": "model.layers.17.mlp.up_proj",
    "146": "model.layers.17.post_attention_layernorm",
    "147": "model.layers.17.self_attn",
    "148": "model.layers.17.self_attn.k_norm",
    "149": "model.layers.17.self_attn.k_proj",
    "150": "model.layers.17.self_attn.o_proj",
    "151": "model.layers.17.self_attn.q_norm",
    "152": "model.layers.17.self_attn.q_proj",
    "153": "model.layers.17.self_attn.v_proj",
    "154": "model.layers.18",
    "155": "model.layers.18.input_layernorm",
    "156": "model.layers.18.mlp",
    "157": "model.layers.18.mlp.act_fn",
    "158": "model.layers.18.mlp.down_proj",
    "159": "model.layers.18.mlp.gate_proj",
    "160": "model.layers.18.mlp.up_proj",
    "161": "model.layers.18.post_attention_layernorm",
    "162": "model.layers.18.self_attn",
    "163": "model.layers.18.self_attn.k_norm",
    "164": "model.layers.18.self_attn.k_proj",
    "165": "model.layers.18.self_attn.o_proj",
    "166": "model.layers.18.self_attn.q_norm",
    "167": "model.layers.18.self_attn.q_proj",
    "168": "model.layers.18.self_attn.v_proj",
    "169": "model.layers.19",
    "170": "model.layers.19.input_layernorm",
    "171": "model.layers.19.mlp",
    "172": "model.layers.19.mlp.act_fn",
    "173": "model.layers.19.mlp.down_proj",
    "174": "model.layers.19.mlp.gate_proj",
    "175": "model.layers.19.mlp.up_proj",
    "176": "model.layers.19.post_attention_layernorm",
    "177": "model.layers.19.self_attn",
    "178": "model.layers.19.self_attn.k_norm",
    "179": "model.layers.19.self_attn.k_proj",
    "180": "model.layers.19.self_attn.o_proj",
    "181": "model.layers.19.self_attn.q_norm",
    "182": "model.layers.19.self_attn.q_proj",
    "183": "model.layers.19.self_attn.v_proj",
    "184": "model.layers.2",
    "185": "model.layers.2.input_layernorm",
    "186": "model.layers.2.mlp",
    "187": "model.layers.2.mlp.act_fn",
    "188": "model.layers.2.mlp.down_proj",
    "189": "model.layers.2.mlp.gate_proj",
    "190": "model.layers.2.mlp.up_proj",
    "191": "model.layers.2.post_attention_layernorm",
    "192": "model.layers.2.self_attn",
    "193": "model.layers.2.self_attn.k_norm",
    "194": "model.layers.2.self_attn.k_proj",
    "195": "model.layers.2.self_attn.o_proj",
    "196": "model.layers.2.self_attn.q_norm",
    "197": "model.layers.2.self_attn.q_proj",
    "198": "model.layers.2.self_attn.v_proj",
    "199": "model.layers.20",
    "200": "model.layers.20.input_layernorm",
    "201": "model.layers.20.mlp",
    "202": "model.layers.20.mlp.act_fn",
    "203": "model.layers.20.mlp.down_proj",
    "204": "model.layers.20.mlp.gate_proj",
    "205": "model.layers.20.mlp.up_proj",
    "206": "model.layers.20.post_attention_layernorm",
    "207": "model.layers.20.self_attn",
    "208": "model.layers.20.self_attn.k_norm",
    "209": "model.layers.20.self_attn.k_proj",
    "210": "model.layers.20.self_attn.o_proj",
    "211": "model.layers.20.self_attn.q_norm",
    "212": "model.layers.20.self_attn.q_proj",
    "213": "model.layers.20.self_attn.v_proj",
    "214": "model.layers.21",
    "215": "model.layers.21.input_layernorm",
    "216": "model.layers.21.mlp",
    "217": "model.layers.21.mlp.act_fn",
    "218": "model.layers.21.mlp.down_proj",
    "219": "model.layers.21.mlp.gate_proj",
    "220": "model.layers.21.mlp.up_proj",
    "221": "model.layers.21.post_attention_layernorm",
    "222": "model.layers.21.self_attn",
    "223": "model.layers.21.self_attn.k_norm",
    "224": "model.layers.21.self_attn.k_proj",
    "225": "model.layers.21.self_attn.o_proj",
    "226": "model.layers.21.self_attn.q_norm",
    "227": "model.layers.21.self_attn.q_proj",
    "228": "model.layers.21.self_attn.v_proj",
    "229": "model.layers.22",
    "230": "model.layers.22.input_layernorm",
    "231": "model.layers.22.mlp",
    "232": "model.layers.22.mlp.act_fn",
    "233": "model.layers.22.mlp.down_proj",
    "234": "model.layers.22.mlp.gate_proj",
    "235": "model.layers.22.mlp.up_proj",
    "236": "model.layers.22.post_attention_layernorm",
    "237": "model.layers.22.self_attn",
    "238": "model.layers.22.self_attn.k_norm",
    "239": "model.layers.22.self_attn.k_proj",
    "240": "model.layers.22.self_attn.o_proj",
    "241": "model.layers.22.self_attn.q_norm",
    "242": "model.layers.22.self_attn.q_proj",
    "243": "model.layers.22.self_attn.v_proj",
    "244": "model.layers.23",
    "245": "model.layers.23.input_layernorm",
    "246": "model.layers.23.mlp",
    "247": "model.layers.23.mlp.act_fn",
    "248": "model.layers.23.mlp.down_proj",
    "249": "model.layers.23.mlp.gate_proj",
    "250": "model.layers.23.mlp.up_proj",
    "251": "model.layers.23.post_attention_layernorm",
    "252": "model.layers.23.self_attn",
    "253": "model.layers.23.self_attn.k_norm",
    "254": "model.layers.23.self_attn.k_proj",
    "255": "model.layers.23.self_attn.o_proj",
    "256": "model.layers.23.self_attn.q_norm",
    "257": "model.layers.23.self_attn.q_proj",
    "258": "model.layers.23.self_attn.v_proj",
    "259": "model.layers.24",
    "260": "model.layers.24.input_layernorm",
    "261": "model.layers.24.mlp",
    "262": "model.layers.24.mlp.act_fn",
    "263": "model.layers.24.mlp.down_proj",
    "264": "model.layers.24.mlp.gate_proj",
    "265": "model.layers.24.mlp.up_proj",
    "266": "model.layers.24.post_attention_layernorm",
    "267": "model.layers.24.self_attn",
    "268": "model.layers.24.self_attn.k_norm",
    "269": "model.layers.24.self_attn.k_proj",
    "270": "model.layers.24.self_attn.o_proj",
    "271": "model.layers.24.self_attn.q_norm",
    "272": "model.layers.24.self_attn.q_proj",
    "273": "model.layers.24.self_attn.v_proj",
    "274": "model.layers.25",
    "275": "model.layers.25.input_layernorm",
    "276": "model.layers.25.mlp",
    "277": "model.layers.25.mlp.act_fn",
    "278": "model.layers.25.mlp.down_proj",
    "279": "model.layers.25.mlp.gate_proj",
    "280": "model.layers.25.mlp.up_proj",
    "281": "model.layers.25.post_attention_layernorm",
    "282": "model.layers.25.self_attn",
    "283": "model.layers.25.self_attn.k_norm",
    "284": "model.layers.25.self_attn.k_proj",
    "285": "model.layers.25.self_attn.o_proj",
    "286": "model.layers.25.self_attn.q_norm",
    "287": "model.layers.25.self_attn.q_proj",
    "288": "model.layers.25.self_attn.v_proj",
    "289": "model.layers.26",
    "290": "model.layers.26.input_layernorm",
    "291": "model.layers.26.mlp",
    "292": "model.layers.26.mlp.act_fn",
    "293": "model.layers.26.mlp.down_proj",
    "294": "model.layers.26.mlp.gate_proj",
    "295": "model.layers.26.mlp.up_proj",
    "296": "model.layers.26.post_attention_layernorm",
    "297": "model.layers.26.self_attn",
    "298": "model.layers.26.self_attn.k_norm",
    "299": "model.layers.26.self_attn.k_proj",
    "300": "model.layers.26.self_attn.o_proj",
    "301": "model.layers.26.self_attn.q_norm",
    "302": "model.layers.26.self_attn.q_proj",
    "303": "model.layers.26.self_attn.v_proj",
    "304": "model.layers.27",
    "305": "model.layers.27.input_layernorm",
    "306": "model.layers.27.mlp",
    "307": "model.layers.27.mlp.act_fn",
    "308": "model.layers.27.mlp.down_proj",
    "309": "model.layers.27.mlp.gate_proj",
    "310": "model.layers.27.mlp.up_proj",
    "311": "model.layers.27.post_attention_layernorm",
    "312": "model.layers.27.self_attn",
    "313": "model.layers.27.self_attn.k_norm",
    "314": "model.layers.27.self_attn.k_proj",
    "315": "model.layers.27.self_attn.o_proj",
    "316": "model.layers.27.self_attn.q_norm",
    "317": "model.layers.27.self_attn.q_proj",
    "318": "model.layers.27.self_attn.v_proj",
    "319": "model.layers.3",
    "320": "model.layers.3.input_layernorm",
    "321": "model.layers.3.mlp",
    "322": "model.layers.3.mlp.act_fn",
    "323": "model.layers.3.mlp.down_proj",
    "324": "model.layers.3.mlp.gate_proj",
    "325": "model.layers.3.mlp.up_proj",
    "326": "model.layers.3.post_attention_layernorm",
    "327": "model.layers.3.self_attn",
    "328": "model.layers.3.self_attn.k_norm",
    "329": "model.layers.3.self_attn.k_proj",
    "330": "model.layers.3.self_attn.o_proj",
    "331": "model.layers.3.self_attn.q_norm",
    "332": "model.layers.3.self_attn.q_proj",
    "333": "model.layers.3.self_attn.v_proj",
    "334": "model.layers.4",
    "335": "model.layers.4.input_layernorm",
    "336": "model.layers.4.mlp",
    "337": "model.layers.4.mlp.act_fn",
    "338": "model.layers.4.mlp.down_proj",
    "339": "model.layers.4.mlp.gate_proj",
    "340": "model.layers.4.mlp.up_proj",
    "341": "model.layers.4.post_attention_layernorm",
    "342": "model.layers.4.self_attn",
    "343": "model.layers.4.self_attn.k_norm",
    "344": "model.layers.4.self_attn.k_proj",
    "345": "model.layers.4.self_attn.o_proj",
    "346": "model.layers.4.self_attn.q_norm",
    "347": "model.layers.4.self_attn.q_proj",
    "348": "model.layers.4.self_attn.v_proj",
    "349": "model.layers.5",
    "350": "model.layers.5.input_layernorm",
    "351": "model.layers.5.mlp",
    "352": "model.layers.5.mlp.act_fn",
    "353": "model.layers.5.mlp.down_proj",
    "354": "model.layers.5.mlp.gate_proj",
    "355": "model.layers.5.mlp.up_proj",
    "356": "model.layers.5.post_attention_layernorm",
    "357": "model.layers.5.self_attn",
    "358": "model.layers.5.self_attn.k_norm",
    "359": "model.layers.5.self_attn.k_proj",
    "360": "model.layers.5.self_attn.o_proj",
    "361": "model.layers.5.self_attn.q_norm",
    "362": "model.layers.5.self_attn.q_proj",
    "363": "model.layers.5.self_attn.v_proj",
    "364": "model.layers.6",
    "365": "model.layers.6.input_layernorm",
    "366": "model.layers.6.mlp",
    "367": "model.layers.6.mlp.act_fn",
    "368": "model.layers.6.mlp.down_proj",
    "369": "model.layers.6.mlp.gate_proj",
    "370": "model.layers.6.mlp.up_proj",
    "371": "model.layers.6.post_attention_layernorm",
    "372": "model.layers.6.self_attn",
    "373": "model.layers.6.self_attn.k_norm",
    "374": "model.layers.6.self_attn.k_proj",
    "375": "model.layers.6.self_attn.o_proj",
    "376": "model.layers.6.self_attn.q_norm",
    "377": "model.layers.6.self_attn.q_proj",
    "378": "model.layers.6.self_attn.v_proj",
    "379": "model.layers.7",
    "380": "model.layers.7.input_layernorm",
    "381": "model.layers.7.mlp",
    "382": "model.layers.7.mlp.act_fn",
    "383": "model.layers.7.mlp.down_proj",
    "384": "model.layers.7.mlp.gate_proj",
    "385": "model.layers.7.mlp.up_proj",
    "386": "model.layers.7.post_attention_layernorm",
    "387": "model.layers.7.self_attn",
    "388": "model.layers.7.self_attn.k_norm",
    "389": "model.layers.7.self_attn.k_proj",
    "390": "model.layers.7.self_attn.o_proj",
    "391": "model.layers.7.self_attn.q_norm",
    "392": "model.layers.7.self_attn.q_proj",
    "393": "model.layers.7.self_attn.v_proj",
    "394": "model.layers.8",
    "395": "model.layers.8.input_layernorm",
    "396": "model.layers.8.mlp",
    "397": "model.layers.8.mlp.act_fn",
    "398": "model.layers.8.mlp.down_proj",
    "399": "model.layers.8.mlp.gate_proj",
    "400": "model.layers.8.mlp.up_proj",
    "401": "model.layers.8.post_attention_layernorm",
    "402": "model.layers.8.self_attn",
    "403": "model.layers.8.self_attn.k_norm",
    "404": "model.layers.8.self_attn.k_proj",
    "405": "model.layers.8.self_attn.o_proj",
    "406": "model.layers.8.self_attn.q_norm",
    "407": "model.layers.8.self_attn.q_proj",
    "408": "model.layers.8.self_attn.v_proj",
    "409": "model.layers.9",
    "410": "model.layers.9.input_layernorm",
    "411": "model.layers.9.mlp",
    "412": "model.layers.9.mlp.act_fn",
    "413": "model.layers.9.mlp.down_proj",
    "414": "model.layers.9.mlp.gate_proj",
    "415": "model.layers.9.mlp.up_proj",
    "416": "model.layers.9.post_attention_layernorm",
    "417": "model.layers.9.self_attn",
    "418": "model.layers.9.self_attn.k_norm",
    "419": "model.layers.9.self_attn.k_proj",
    "420": "model.layers.9.self_attn.o_proj",
    "421": "model.layers.9.self_attn.q_norm",
    "422": "model.layers.9.self_attn.q_proj",
    "423": "model.layers.9.self_attn.v_proj",
    "424": "model.norm",
    "425": "model.rotary_emb"
}