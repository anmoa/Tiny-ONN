# Tiny-ONN: A Decoupled Meta-Learning Framework

[![Theory: IPWT](https://img.shields.io/badge/Theory-IPWT-blue)](https://github.com/dmf-archive/IPWT)
[![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)
[![Status: Training](https://img.shields.io/badge/Status-Training%20Hyper--SMoE-green)](./docs/devlog/model_card.md)

> "We are not just training a model. We are training a model to learn how to learn more efficiently."

---

## What is Tiny-ONN?

**Tiny-ONN (Tiny Ouroboros Neural Network)** is a research project to build a new generation of AI systems based on the principle of **decoupled meta-learning**. We aim to move beyond monolithic, black-box architectures towards models that can dynamically route computation and specialize their knowledge, enabling greater efficiency and a stronger defense against catastrophic forgetting.

Our core hypothesis is that a truly intelligent system requires two distinct learning processes:
1.  A **Fast System (The Experts)** that directly learns to solve tasks and minimize prediction errors.
2.  A **Slow System (The Router)** that meta-learns how to route problems to the correct experts, optimizing for the lowest possible **learning cost (Surprise)**.

This "fast/slow thinking" paradigm, inspired by cognitive science and our foundational **[IPWT (Integrated Predictive Workspace Theory)](https://github.com/dmf-archive/IPWT)**, allows the model to physically isolate knowledge domains within its architecture, leading to more robust and efficient learning.

## Architecture: Hierarchical Mixture-of-Experts (HMoE)

To realize this vision, Tiny-ONN employs a **Hierarchical Mixture-of-Experts (HMoE)** architecture that replaces the standard Feed-Forward/MLP layers in the Transformer block.

-   **Dynamic & Hierarchical Routing**: We use a two-level gating mechanism inspired by `DynMoE`'s `GAMoEGateT`. For each input token, a Level 1 router first selects a group of experts, and then a Level 2 router selects specific experts within that group. The number of activated experts is dynamic, allowing the model to allocate more computational resources to more complex tokens.
-   **Hyper-Sparse Specialization**: Our current implementation experiments with a large number of extremely lightweight experts, shifting complexity from individual expert capacity to the routing network's intelligence.

## Training Paradigm: Decoupled Meta-Learning

The key innovation of Tiny-ONN is its training loop, which separates the optimization of the fast and slow systems.

1.  **Phase 1: Expert Learning (Fast System)**
    -   The router's weights are frozen.
    -   The model performs a standard forward pass to generate predictions.
    -   A primary loss (`L_main`, e.g., cross-entropy) is calculated and backpropagated.
    -   Only the weights of the activated experts are updated to minimize the prediction error.
    -   During this backpropagation, we use `backward_hooks` to intercept and cache the **gradient norm (Surprise)** generated by each token on its activated expert.

2.  **Phase 2: Gating Meta-Learning (Slow System)**
    -   The experts' weights are frozen.
    -   We use the cached data from Phase 1. The "ground truth" for the router is the expert path that produced the **minimum surprise**.
    -   A secondary loss (`L_router`) is calculated to train the router to predict this minimum-surprise path for any given input.
    -   Only the router's weights are updated.

This dual-optimizer, dual-backward pass approach ensures the learning signals are pure and allows the router to focus exclusively on its meta-task: becoming an efficient "manager" that minimizes the entire system's learning effort.

## Current Implementation: Tiny-ONN 0.6B Hyper-SMoE

We are currently focused on training and evaluating our first flagship model, designed to be trainable on a single consumer-grade GPU (e.g., RTX 2070 8GB).

| Hyperparameter          | Value          | Notes                                        |
| :---------------------- | :------------- | :------------------------------------------- |
| **Base Model**          | `Qwen/Qwen3-0.6B` | Attention and Embedding layers are inherited & frozen. |
| **Total Parameters**    | **~0.51 B**    | Backbone + 896 Experts.                      |
| `num_experts_per_layer` | **32**         | Total MoE params are equivalent to the original MLP. |
| `moe_intermediate_size` | **64**         | Creates extremely lightweight experts (~0.2M params each). |
| **Training Strategy**   | **Online Distillation** | A 4-bit quantized `Qwen3-0.6B` acts as a live teacher. |

This setup drastically reduces computational requirements, as we only need to train the MoE and router layers from scratch, guided by a powerful, pre-trained teacher model.

## Installation & Usage

The project is in a heavy research and development phase. The primary entry point is the `train.py` script.

To set up the environment:

```bash
# It is highly recommended to use uv for environment management
# https://github.com/astral-sh/uv
uv venv
source .venv/bin/activate
uv sync
```

To run the training script:

```bash
# Run training using a configuration file
python train.py --config_file configs/meta_train_v1.yaml
```

---

## Citation

If you find our theoretical framework or preliminary tools useful in your research, please consider citing our foundational work:

```bibtex
@misc{ipwt2025,
  author       = {Rui, L.},
  title        = {{Integrated Predictive Workspace Theory: Towards a Unified Framework for the Science of Consciousness}},
  year         = {2025},
  publisher    = {Zenodo},
  doi= {10.5281/zenodo.15676304},
  url= {https://doi.org/10.5281/zenodo.15676304}
}
```

## License

This project is licensed under the AGPLv3 License. See the `LICENSE` file for details.
