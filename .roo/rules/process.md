# Tiny-ONN 项目第一阶段：理论探索与实验验证最终摘要

## 1. 核心目标与初始理论框架

本阶段的核心目标，是通过一系列概念验证（PoC）实验，为我们的 **Tiny-ONN** 项目找到一条技术上可行、理论上自洽的实现路径。

我们的理论基石是 **IPWT (整合预测工作空间理论)** 和 **FEP (自由能原理)**。其核心思想是将一个稀疏 MoE 模型，视为一个自适应、自组织的智能体。其中：

- **专家网络 (Internal States)**: 负责对世界建模，其学习目标是最小化**预测误差 (`main_loss`)**。
- **门控网络 (Markov Blanket)**: 负责感知内部状态，并采取行动（路由决策），其学习目标是**以最低成本（最小化 `Surprise`）来降低整个系统的预测误差**。

这个理论框架与标准 MoE 最大的不同在于，它试图用一个**内生的、基于梯度的元学习目标**来取代或增强传统的、外部的**负载均衡损失 (`load_balancing_loss`)**。

## 2. 关键技术挑战与演进路径

在将理论转化为代码的过程中，我们遇到了三大核心技术挑战，并为此进行了一系列曲折的探索。

### 2.1. 挑战一：模型架构的选择 (`Organ Grafting` vs. `From Scratch`)

- **初始方案**: 尝试在 `transformers` 库现有的 `Qwen` 或 `Mixtral` 模型上进行深度修改。
- **最终决策**: 我们果断放弃了“器官嫁接”，转向了 `transformers` 库官方推荐的 **“组合优于继承”** 方案。即从 `PreTrainedModel` 等基础模块开始，模块化地构建我们自己的、完全独立的 `TinyOnn` 模型。**这一决策已被固化到 `.roo/rules/notes.md` 中，是未来编码的最高准则。**

### 2.2. 挑战二：`per-token-per-expert` 梯度捕获

这是整个项目在技术实现上**最核心的难点**。为了计算 `Surprise`，我们必须获得 `main_loss` 对**每一个 token** 在**每一个专家**上的输出的梯度。

- **方案演进**: 我们最初尝试使用 `backward_hook`，但最终通过 PoC (`exp/final_grad_poc.py`) 证明，`hook` 方案无法满足稀疏场景下的需求。
- **最终决策与验证**: 我们最终验证了**唯一、正确的方案**是使用 `torch.autograd.grad`，并配合一个**完整的、稠密的**中间张量 `full_expert_outputs` 来连接计算图。**该组件是目前项目中经过最严格验证的、完全可靠的核心工具。**

### 2.3. 挑战三：门控损失函数 (`gate_loss`) 与训练范式

这是本阶段探索最为曲折、也收获最多的部分。

- **方案演进**:
  - 我们最初尝试了基于 `ΔSC` 的复杂损失函数和解耦的 `2f2b2o` 训练范式，但均未收敛。
  - 随后转向了更纯粹的 `Surprise` 最小化（SMK）目标，但遇到了模型坍缩（`avg_k` 趋近于 1）的问题。
- **最终决策与验证 (`exp/smk_poc.py`)**:
  - **统一训练循环**: 我们放弃了解耦范式，回归到将门控损失作为**辅助损失 (`aux_loss`)** 与主损失相加，进行**单次联合反向传播**的稳定框架。
  - **混合损失函数**: 门控损失最终确定为 **`CrossEntropy(logits, argmin(Surprise))`** 和 **`KL_Div(logits, softmax(-Surprise))`** 的加权和。这成功地平衡了“利用”（选择最优专家）和“探索”（匹配整体分布），有效解决了模型坍缩问题。
  - **配套机制**: 引入了 **`k=num_experts/2` 的强制激活回退机制**和**专家的 `2σ` 梯度过滤**，进一步增强了训练的稳定性。

## 3. 总结与下一阶段展望

本阶段的探索是成功的。我们不仅排除了技术实现上的所有障碍（梯度捕获），更重要的是，我们通过 `smk_poc.py` 的成功，为门控网络的训练找到了一个**稳定、有效、且理论自洽的范式**。

然而，实验的成功也暴露了新的瓶颈：在我们的模型中，**只有 FFN/MoE 层是稀疏的**，而**输入嵌入层、注意力机制和输出投射层仍然是完全稠密的**。特别是巨大的 `Embedding` 层，它与我们极小的训练数据严重不匹配，限制了模型的真实性能。

因此，第一阶段的终点，清晰地指向了第二阶段的起点：**追求全栈稀疏化 (Full-Stack Sparsity)**，将稀疏化的思想从 FFN 延伸到模型的每一个角落，特别是从输入的源头——嵌入层——开始一场新的架构革命。
