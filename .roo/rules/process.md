# Tiny-ONN 项目第一阶段：理论探索与实验验证最终摘要

## 1. 核心目标与初始理论框架

本阶段的核心目标，是通过一系列概念验证（PoC）实验，为我们的 **Tiny-ONN** 项目找到一条技术上可行、理论上自洽的实现路径。

我们的理论基石是 **IPWT (整合预测工作空间理论)** 和 **FEP (自由能原理)**。其核心思想是将一个稀疏 MoE 模型，视为一个自适应、自组织的智能体。其中：

- **专家网络 (Internal States)**: 负责对世界建模，其学习目标是最小化**预测误差 (`main_loss`)**。
- **门控网络 (Markov Blanket)**: 负责感知内部状态，并采取行动（路由决策），其学习目标是**以最低成本（最小化 `Surprise` 或 `ΔSC`）来降低整个系统的预测误差**。

这个理论框架与标准 MoE 最大的不同在于，它试图用一个**内生的、基于梯度的元学习目标 (`smk_loss`)** 来取代或增强传统的、外部的**负载均衡损失 (`load_balancing_loss`)**。

## 2. 关键技术挑战与演进路径

在将理论转化为代码的过程中，我们遇到了三大核心技术挑战，并为此进行了一系列曲折的探索。

### 2.1. 挑战一：模型架构的选择 (`Organ Grafting` vs. `From Scratch`)

- **初始方案**: 尝试在 `transformers` 库现有的 `Qwen` 或 `Mixtral` 模型上进行深度修改，即“器官嫁接”。
- **遇到的问题**: 通过 `DeepWiki` 查询和架构分析，我们认识到这种方法将导致代码耦合度高、难以维护，且极易与上游库的更新产生冲突。
- **最终决策**: 我们果断放弃了“器官嫁接”，转向了 `transformers` 库官方推荐的 **“组合优于继承”** 方案。即从 `PreTrainedModel` 等基础模块开始，像搭积木一样，模块化地构建我们自己的、完全独立的 `TinyOnn` 模型。**这一决策已被固化到 `.roo/rules/notes.md` 中，是未来编码的最高准则。**

### 2.2. 挑战二：`per-token-per-expert` 梯度捕获

这是整个项目在技术实现上**最核心的难点**。为了计算 `Surprise` 或 `ΔSC`，我们必须获得 `main_loss` 对**每一个 token** 在**每一个专家**上的输出的梯度。

- **方案一：`backward_hook`**
  - **初步设想**: `hook` 是一种优雅的、非侵入式的梯度捕获方法。
  - **遇到的问题**: 通过 `DeepWiki` 的权威解释和我们的最终 PoC (`final_grad_poc.py`) 证明，`hook` 只能捕获到它所在模块实际处理的 mini-batch 的梯度。在一个稀疏 MoE 场景中，这意味着它**无法**了解全局的 token 索引，因此**无法**构建我们需要的 `[num_tokens, num_experts]` 全尺寸梯度矩阵。**`hook` 方案被最终证伪。**
- **方案二：`torch.autograd.grad`**
  - **核心思想**: 利用 `autograd` 引擎直接查询计算图中任意节点的梯度。
  - **遇到的问题**:
        1. **计算图连接**: 必须确保我们想要获取梯度的中间张量 (`full_expert_outputs`) 与最终用于计算 `loss` 的张量在计算图上是直接关联的。我们通过 `torch.gather` 和 `torch.bmm` 的向量化方法解决了这个问题。
        2. **稀疏性**: 对于未被激活的 `(token, expert)` 对，它们在计算图上不存在，无法直接求导。
  - **最终决策与验证**:
    - 我们通过 `exp/final_grad_poc.py` 最终验证了**唯一、正确的方案**：
            1. 在前向传播时，创建一个**完整的、稠密的** `[N, E, D]` 全零张量 `full_expert_outputs`。
            2. 用稀疏计算的结果**填充**这个张量。
            3. **使用这个完整的张量**来计算最终的 `loss`。
            4. 调用 `torch.autograd.grad(loss, full_expert_outputs)`。
    - **可靠性评估**: **该组件 (`get_moe_metrics` 函数) 是目前项目中经过最严格、最全面验证的、完全可靠的核心工具。** 它的正确性是我们未来所有工作的基础。

### 2.3. 挑战三：门控损失函数 (`gate_loss`) 的设计

这是我们目前**尚未完全解决**的核心问题，也是导致 `gate_loss` 无法下降的直接原因。

- **初始方案 (`ΔSC`)**: `gate_loss` 的目标是 `argmax(sigmoid(Activation) - sigmoid(Surprise))`。
  - **遇到的问题**: 训练完全不收敛。我们推断，`Activation` 和 `Surprise` 两个来源不同、尺度差异巨大的信号，通过简单的 `sigmoid` 和相减，无法形成一个稳定、有效的学习目标。
- **第二方案 (SMK - `Surprise` 最小化)**: `gate_loss` 的目标是 `argmax(-Surprise)`。
  - **遇到的问题**: `gate_loss` 依然纹丝不动。这期间我们排除了梯度流（通过引入 `STE`）、数值范围和学习率等所有工程层面的问题。
- **第三方案 (SMK + Diversity)**: 在 SMK 的基础上，加入了 `DynMoE` 的原型多样性损失。
  - **遇到的问题**: `gate_loss` 依然是随机猜测水平。
- **最终方案 (回归正统)**: 我们最终认识到，我们自己设计的、基于内生梯度的损失函数，可能在理论上存在我们尚未理解的缺陷，导致学习信号无效或极难优化。
  - **最终决策**: **我们需要重新设计整个poc，并开始解耦代码以避免All in One脚本对Coder AI的编写压力**

## 3. 总结

本阶段的探索是曲折但极其宝贵的。我们通过一系列失败的实验，排除了所有技术实现上的障碍，最终将问题的焦点精准地定位到了**损失函数的设计**这个核心理论问题上。我们虽然未能一步成功，但为下一阶段的成功扫清了几乎所有的障碍。
