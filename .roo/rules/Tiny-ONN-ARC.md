# 技术日志：`Tiny-ONN-ARC` 的演进之路 (v0.8.0)

## 1. v0.5.0 的失败：Teacher Forcing 的陷阱

我们最初的 v0.5.0 范式，采用了标准的 Teacher Forcing。实验结果无情地揭示了这个范式的根本缺陷。模型学会的只是局部的启发式规则，在自回归生成时立刻陷入 **“模式崩溃” (Mode Collapse)**。

**结论**：`main_loss` 是一个**短视的、局部的**信号，无法迫使模型进行真正的、全局的推理。这是典型的 **“暴露偏差” (Exposure Bias)** 问题。

## 2. v0.6.0：EAVI——理论正确的对齐范式

在 v0.5.0 的失败之后，我们确立了 `EAVI (Excursion-Alignment Variational Inference)` 范式，作为解决 ARC 任务的理论正确道路。

> **为了让模型学会真正的推理，它必须在训练中直面自己独立生成完整序列（一次认知远足 Excursion）所带来的全部后果。`main_loss` 必须是基于这个完整生成序列与地面真值（Alignment）的、全局的、无法作弊的对齐信号。**

EAVI 的联合优化目标 (`main_loss` + `gating_loss`) 本质上是在最小化整个系统的变分自由能 (VFE)。

## 3. v0.7.0：架构革命——解耦“感知”与“推理”

在 v0.6.0 的 EAVI 框架下，我们遭遇了训练崩溃。日志显示，`gating_loss` 极高而 `GateAcc` 接近于零，模型陷入了“尝试预测一个随机信号”的恶性循环。

### 3.1. 核心问题：孔径问题 (Aperture Problem)

我们诊断出问题的根源：模型在进行路由决策时，缺乏足够的局部上下文。一个像素级的路由网络无法“看见”它所属的宏观形状，就像通过一个小孔观察移动的物体，无法判断其整体运动方向。

### 3.2. 架构 `v2.0`：对象发现层 (Object Finder Layer)

为了解决这个问题，我们进行了一次深刻的架构革命，其核心思想是**解耦“结构感知”与“规则应用”**。

1. **对象发现层 (OFL)**: 我们在模型的最前端（Embedding 之后）引入了一个特殊的、**非因果的全局 `DynSMHALayer`**。它的唯一任务，就是计算所有像素之间的亲和度，将像素动态地分组为“对象”，并将这些对象的全局上下文信息（原型）广播回给每个像素。
2. **无注意力的 `SimplifiedBlock`**: 在 OFL 完成了最困难的“结构感知”任务后，后续的 Transformer Block 不再需要 `DynSMHA` 进行特征提取。我们将其简化为只包含 `LayerNorm`、`DynMoE` 和残差连接的高效计算块。它的任务是在给定的对象上下文中，为像素选择并应用正确的“变换规则”（MoE 专家）。

这个新架构为模型提供了一个极其强大的归纳偏置：“**先理解场景的结构，再对结构中的每个部分应用规则**”，并极大地提升了计算效率。

## 4. v0.8.0：联邦 EAVI——优化范式的最终形态

在架构 `v2.0` 的基础上，我们通过一系列严谨的 PoC 实验，最终发现了之前 EAVI 训练失败的、更深层次的原因，并确立了最终的优化范式。

### 4.1. 致命缺陷：聚合更新破坏了元学习

我们发现，标准的、对整个批次（batch）进行聚合更新的优化流程，与 EAVI 的元学习本质是根本矛盾的。

`gating_loss` 的学习目标是动态计算出的 `surprise`（即 `main_loss` 的梯度）。形式化地，我们证明了：

`gating_loss( grad( mean(L_i) ) ) ≠ mean( gating_loss( grad(L_i) ) )`

- **左边（旧的聚合更新）**: 让门控网络去学习如何为一个**不存在的、平均化的“幽灵任务”**进行路由，其学习信号被严重污染。
- **右边（新的逐 `seq` 更新）**: 为门控网络提供了多个**清晰、独立、真实的**学习样本，告诉它“对于任务 A，应该这样路由；对于任务 B，应该那样路由”。

### 4.2. “逐 `seq` 更新”范式

为了保留元学习循环的**即时性 (immediacy)** 和信息保真度，我们最终确定，**必须在处理完每一个序列后，立刻执行一次完整的优化步骤**。

我们重构了训练循环，使其在一个 `for` 循环中遍历批次里的每一个序列，并**立即**执行 `backward()` 和 `optimizer.step()`。我们还通过 PoC 实验意外地发现，这种方法在我们的用例中，由于避免了对超大计算图进行 `autograd.grad`，其性能**反而比聚合更新更高**。

至此，我们构建了理论正确、工程可行且性能优越的最终训练范式，我们称之为 **“联邦 EAVI”**。

## 5. EAVI 与主流对齐方法对比

下表将我们最终的 **EAVI** 与其他主流 LLM 对齐方法进行了对比：

| 特性 | PPO (Proximal Policy Optimization) | DPO (Direct Preference Optimization) | **联邦 EAVI (Federated EAVI)**   |
| :---- | :-- | :---- | :-- |
| **核心机制**| 强化学习 (RL)，最大化奖励   | 直接优化偏好对的概率   | **元学习**，最小化每个独立生成序列与真值的**变分自由能**|
| **需要奖励模型**   | 是| 否| **否** |
| **需要参考模型**   | 是| 是| **否** (自回归生成即为探索) |
| **训练流程**| 多阶段 | 多阶段   | **单阶段**  |
| **优化单元**| 批次 (Batch)  | 偏好对 (Pair)   | **序列 (Sequence)**|
| **解决的核心问题** | 优化一个复杂的、不可微的目标| 稳定地拟合人类偏好| **迫使模型放弃局部模式匹配，为每一个独立的推理任务学会高效、正确的内部计算路径（路由）** |
