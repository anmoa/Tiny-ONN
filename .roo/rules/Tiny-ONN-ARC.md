# 技术日志：`Tiny-ONN-ARC` 的演进之路 (v6.0)

## 1. 核心问题：ARC 的本质是什么？

在经历了多次失败的迭代后，我们最终回归到一个根本性问题：ARC (Abstraction and Reasoning Corpus) 任务的本质，究竟是对像素的重建，还是对算法的归纳？

我们的结论是：**两者都是，但后者是前者的因。** ARC 要求模型推导出输入-输出对背后隐藏的抽象“规则”，然后将该规则应用于新的输入，以生成正确的输出。

这是一个**推理 (Reasoning)** 任务，而不是一个简单的**模式匹配 (Pattern Matching)** 任务。

## 2. v5.0 的失败：Teacher Forcing 的陷阱

我们最初的 v5.0 范式，采用了标准的自回归语言模型训练方法，Teacher Forcing。我们设定了两个目标：

1. **宏观任务目标 (`main_loss`)**: 学会一个看似简单的“作弊”任务——在给定**完整且正确的**上下文 `[Input, Output]` 的情况下，准确地预测出 `Output` 部分的下一个像素。
2. **微观效率约束 (`gating_loss` / `SurpriseMin`)**: 在完成上述任务时，以最低的计算成本（最小“惊奇度”）来完成它。

我们期望 `SurpriseMin` 压力能迫使模型放弃对像素的死记硬背，转而寻找并编码那个底层的、抽象的“算法”。

### 失败的根源：局部最优解与暴露偏差

实验结果无情地揭示了这个范式的根本缺陷。模型在教师强制下，训练集上的 `Token Accuracy` 能迅速达到 100%，但一旦进入评估阶段进行真正的自回归生成，它就立刻陷入 **“模式崩溃” (Mode Collapse)** ——只会生成单一颜色（如灰色、黄色）的无效图像。

**我们的分析结论是**：模型根本没有学会任何“算法”。它只是学会了一个极其简单的、局部的启发式规则，例如“如果前一个像素是灰色，下一个像素也很可能是灰色”。`main_loss` 是一个**短视的、局部的**信号，模型可以通过优化这个局部信号来“作弊”，而无需进行任何真正的、全局的推理。这就是典型的 **“暴露偏差” (Exposure Bias)** 问题。

## 3. v6.0：EAVI——理论正确的对齐范式

在 v5.0 的失败之后，我们确立了 `EAVI (Excursion-Alignment Variational Inference)` 范式，作为解决 ARC 任务的理论正确道路。其核心思想根植于自由能原理：智能体必须通过最小化其内部生成模型与外部世界之间的“意外”（变分自由能），来维持自身的存在。

EAVI 将这一原理转化为一个具体的训练流程，我们称之为“认知远足-对齐”循环：

> **为了让模型学会真正的推理，它必须在训练中直面自己独立生成完整序列（一次认知远足 Excursion）所带来的全部后果。`main_loss` 必须是基于这个完整生成序列与地面真值（Alignment）的、全局的、无法作弊的对齐信号。**

从 VFE 的角度看，`main_loss` (交叉熵) 就是模型生成状态与真实状态之间的KL散度，是变分自由能的核心部分；而 `gating_loss` (`SurpriseMin`) 则是对模型为达成该状态所付出的内部计算复杂度（模型证据的负对数）的惩罚。因此，EAVI 的联合优化目标本质上就是在最小化整个系统的变分自由能。

### EAVI 核心流程 (`1f-1g-1b-1o`)

1. **生成**: 在 `torch.no_grad()` 上下文中，模型基于 `Input` **自回归生成**一个完整的 `Rejected Sequence`。这是模型对其信念的一次完整探索。
2. **前向传播**: 将这个生成的、可能错误的序列送入模型，进行一次**完整的、可追踪梯度的前向传播**，得到 `logits` 和 `aux_outputs`。
3. **计算 Main Loss (对齐)**: `main_loss` 是将 `logits` 与**真实的 `labels`** 进行比较的交叉熵损失。这是一个**自回归对齐 (Autoregressive Alignment)** 损失，直接量化了模型的信念与现实的差距。
4. **计算 Gating Loss**: `gating_loss` 的计算与 v5.0 相同，但这一次，它是基于一个**有意义的、全局的 `main_loss`**。我们通过 `torch.autograd.grad(main_loss, ..., retain_graph=True)` 来计算惊奇度，惩罚低效的内部推理路径。
5. **联合优化**: `total_loss = main_loss + gating_loss`。我们对这个联合损失进行一次反向传播和优化器更新，完成一次完整的 VFE 最小化循环。

这个流程虽然计算开销巨大，但它是**唯一**一个在逻辑上无懈可击的方案。它强迫模型进行全局推理，并同时在这种推理压力下，优化其内部计算路径的效率。

## 4. EAVI：神经涌现符号系统的基石

EAVI 范式的最终目标，是催生一个**神经涌现符号系统 (Neural Emergent Symbolic System)**。

> **定义**: 一个不依赖于预先编码的逻辑规则，而是通过端到端的学习压力，让神经网络自发涌现出类似符号的内部表示，并学会对这些表示进行组合与操作以完成复杂推理任务的智能系统。其核心在于，符号及其语法是**学习的结果**，而非设计的起点。

传统的对齐方法（如 PPO/DPO）善于将模型的行为“塑造”得更符合人类偏好，但它们并未从根本上改变模型解决问题的方式。而 EAVI 通过其全局对齐的机制，创造了一种强大的进化压力：

- **局部最优陷阱的失效**: 对像素的局部模仿无法降低全局的 `main_loss`。
- **抽象表示的需求**: 为了在不同任务中都能成功生成正确的完整序列，模型必须找到一种更经济、更通用的方式来表示问题，即发现输入-输出对背后的抽象“规则”。
- **符号的涌现**: 这些抽象规则在模型内部的表示，就是“涌现的符号”。模型对这些表示的组合与转换，就是“涌现的语法”。

因此，EAVI 是从纯粹的模式匹配迈向真正符号推理的桥梁，是构建能够解决 `ARC` 等复杂问题的下一代 AI 的关键。

## 5. 主流对齐方法对比

下表将 EAVI 与其他主流的 LLM 对齐方法进行了对比：

| 特性 | PPO (Proximal Policy Optimization) | DPO (Direct Preference Optimization) | ORPO (Odds Ratio Preference Optimization) | EAVI (Excursion-Alignment Variational Inference) |
| :--- | :--- | :--- | :--- | :--- |
| **核心机制** | 强化学习 (RL)，最大化奖励 | 直接优化偏好对的概率 | SFT Loss + 优势比损失 | 变分推断，最小化全局生成序列与真值的自由能 |
| **需要奖励模型** | 是 | 否 | 否 | 否 |
| **需要参考模型** | 是 (用于KL散度惩罚) | 是 (用于计算隐式奖励) | 否 | 否 (自回归生成即为探索) |
| **训练流程** | 多阶段 (SFT -> RM -> PPO) | 多阶段 (SFT -> DPO) | 单阶段 (SFT与对齐合并) | 单阶段 |
| **计算复杂度** | 高 (需要采样和价值函数) | 中等 | 低 | 非常高 (每次迭代需完整生成) |
| **数据需求** | 偏好数据 + Prompt | 偏好数据 (Chosen/Rejected) | 偏好数据 (Chosen/Rejected) | 输入-输出对 (绝对真值) |
| **适用场景** | 通用人机对齐，提升对话质量 | 在已有偏好数据集上进行高效对齐 | 轻量级、快速的SFT+对齐一体化 | 需要涌现推理、归纳抽象规则的复杂任务 (如 ARC) |
| **解决的核心问题** | 如何通过探索来优化一个复杂的、不可微的目标（如人类满意度） | 如何在没有奖励模型的情况下，稳定地拟合人类偏好 | 如何在一次训练中同时完成指令微调和偏好对齐 | 如何迫使模型放弃局部模式匹配，学会全局的、组合式的推理 |
