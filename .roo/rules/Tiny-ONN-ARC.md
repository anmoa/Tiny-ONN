# 技术日志：`Tiny-ONN-ARC` 的演进之路 (v6.0)

## 1. 核心问题：ARC 的本质是什么？

在经历了多次失败的迭代后，我们最终回归到一个根本性问题：ARC (Abstraction and Reasoning Corpus) 任务的本质，究竟是对像素的重建，还是对算法的归纳？

我们的结论是：**两者都是，但后者是前者的因。** ARC 要求模型推导出输入-输出对背后隐藏的抽象“规则”，然后将该规则应用于新的输入，以生成正确的输出。

这是一个**推理 (Reasoning)** 任务，而不是一个简单的**模式匹配 (Pattern Matching)** 任务。

## 2. v5.0 的失败：Teacher Forcing 的陷阱

我们最初的 v5.0 范式，采用了标准的自回归语言模型训练方法，Teacher Forcing。我们设定了两个目标：

1. **宏观任务目标 (`main_loss`)**: 学会一个看似简单的“作弊”任务——在给定**完整且正确的**上下文 `[Input, Output]` 的情况下，准确地预测出 `Output` 部分的下一个像素。
2. **微观效率约束 (`gating_loss` / `SurpriseMin`)**: 在完成上述任务时，以最低的计算成本（最小“惊奇度”）来完成它。

我们期望 `SurpriseMin` 压力能迫使模型放弃对像素的死记硬背，转而寻找并编码那个底层的、抽象的“算法”。

### 失败的根源：局部最优解与暴露偏差

实验结果无情地揭示了这个范式的根本缺陷。模型在教师强制下，训练集上的 `Token Accuracy` 能迅速达到 100%，但一旦进入评估阶段进行真正的自回归生成，它就立刻陷入 **“模式崩溃” (Mode Collapse)** ——只会生成单一颜色（如灰色、黄色）的无效图像。

**我们的分析结论是**：模型根本没有学会任何“算法”。它只是学会了一个极其简单的、局部的启发式规则，例如“如果前一个像素是灰色，下一个像素也很可能是灰色”。`main_loss` 是一个**短视的、局部的**信号，模型可以通过优化这个局部信号来“作弊”，而无需进行任何真正的、全局的推理。这就是典型的 **“暴露偏差” (Exposure Bias)** 问题。

## 3. v6.0：走向理论正确的训练范式

在 v5.0 的失败之后，我们进行了一系列痛苦但深刻的探索，以寻找一个能从根本上解决问题的训练范式。

### 探索之一：看似有用的性能优化为何是死路？

我们意识到，所有问题的根源，都指向了那个在训练循环中实时进行的、极其缓慢的**自回归生成**步骤。为了加速，我们探索了所有可能的“捷径”：

1. **异步惊奇度 (Asynchronous Surprise)**: 无论是基于一步延迟的梯度钩子，还是基于参数变化量（KL 散度代理），我们都发现，将惊奇度的计算与当前的梯度流解耦，会破坏 `SurpriseMinLoss` 的核心逻辑链条。门控网络将无法收到关于“如何路由才能更好地降低 `main_loss`”的有效信号。**结论：惊奇度必须与主损失在同一个计算图中同步计算。**

2. **自动编译 (`torch.compile`)**: 这是我们最有希望的“银弹”，但最终被平台和硬件的兼容性问题扼杀。`torch.compile` 的 GPU 后端依赖 Triton，而 Triton 在我们的 Windows 环境下存在致命的兼容性问题。强行适配需要巨大的环境降级成本和技术妥协。**结论：不能依赖通用自动优化工具来解决我们这种高度定制化模型的根本瓶颈。**

3. **Teacher Forcing 变体 (N+2 预测)**: 我们尝试了让模型预测更远的未来，以期引入更强的长程依赖信号。实验再次失败，模型依然陷入“模式崩溃”。**结论：任何形式的、只提供部分正确上下文的 Teacher Forcing，都无法从根本上解决暴露偏差。**

### v6.0 方案：接受复杂性，回归唯一正确的道路

所有探索都将我们带回了同一个、不可回避的结论：

> **为了让模型学会真正的推理，它必须在训练中直面自己独立生成完整序列所带来的全部后果。`main_loss` 必须是基于这个完整生成序列的、全局的、无法作弊的对齐信号。**

我们最终确立的 v6.0 范式，是唯一理论正确的方案。我们不再试图绕过它的计算复杂性，而是接受它，并致力于在实现层面进行优化。

**核心流程 (`1f-1g-1b-1o`)**:

1. **生成 (无梯度)**: 在 `torch.no_grad()` 上下文中，模型基于 `Input` **自回归生成**一个完整的 `Rejected Sequence`。
2. **前向传播**: 将这个生成的、可能错误的序列送入模型，进行一次**完整的、可追踪梯度的前向传播**，得到 `logits` 和 `aux_outputs`。
3. **计算 Main Loss**: `main_loss` 是将 `logits` 与**真实的 `labels`** 进行比较的交叉熵损失。这是一个**自回归对齐 (Autoregressive Alignment)** 损失。
4. **计算 Gating Loss**: `gating_loss` 的计算与 v5.0 相同，但这一次，它是基于一个**有意义的、全局的 `main_loss`**。我们通过 `torch.autograd.grad(main_loss, ..., retain_graph=True)` 来计算惊奇度。
5. **联合优化**: `total_loss = main_loss + gating_loss`。我们对这个联合损失进行一次反向传播和优化器更新。

这个流程虽然计算开销巨大（特别是 `autograd.grad` 和 `retain_graph=True`），但它是**唯一**一个在逻辑上无懈可击的方案。它强迫模型进行全局推理，并同时在这种推理压力下，优化其内部计算路径的效率。这是我们走向真正智能的、唯一可行的道路。
