# Tiny-ONN 开发计划与技术参考

**版本：** 2.0
**日期：** 2025-07-10

## 摘要

本开发计划详细阐述了 Tiny-ONN (Tiny-Optimized Neural Network) 的构建、训练与评估策略。Tiny-ONN 旨在通过一种创新的“知识提取”工作流，从大型预训练语言模型（如 Qwen3-1.7B）中外科手术式地提取并重组核心知识单元，形成一个由稀疏专家模块和动态路由系统组成的紧凑、高效且可解释的模型。作为高度自主的AI架构师和工程专家，我们将自主规划、编写、测试、调试和优化所有必要的代码，以实现Tiny-ONN的开发、训练和评估。

## 1. 引言

### 1.1 背景与动机

- **大型模型的困境**：当前的大型语言模型（LLMs）虽然能力强大，但其巨大的参数量带来了高昂的计算成本、内存占用和部署复杂性。同时，其“黑箱”特性限制了在关键领域的可信度。
- **稀疏化的前景**：稀疏专家模型（SMoE）通过条件计算，在降低计算量的同时保持甚至提升模型容量，提供了一个有前景的解决方案。
- **Tiny-ONN 的创新**：我们提出一种“知识提取”方法，不再从零开始训练稀疏模型，而是从一个已充分预训练的稠密 LLM 中，根据其在实际任务中的行为，动态地、外科手术式地提取核心知识单元，作为高质量的初始专家模块。

### 1.2 Tiny-ONN 核心理念

- **知识提取 (Knowledge Extraction)**：通过对 Qwen3-1.7B 在 SFT 任务上的激活与梯度模式进行精细化分析，量化每个参数的“协同贡献度”。
- **动态选择性抽样 (Dynamic Selective Sampling)**：基于“高激活 + 低梯度 = 高协同”原则，动态选择和组装参数，而非依赖随机初始化或预设稀疏性。
- **稀疏专家重组 (Sparse Expert Recomposition)**：将提取出的、功能上高度相关的参数群（“概念细胞”）重组为 Tiny-ONN 的专家模块。
- **自适应路由 (Adaptive Routing)**：设计一个全新且独立的路由系统，与专家模块协同进化，实现智能的知识调度。
- **可解释性 (Interpretability)**：通过分析和标注专家的功能，揭示模型内部的知识分区和决策机制。

## 阶段 0: 数字 fMRI (Digital fMRI)

此阶段的目标是构建一个数字 fMRI 扫描仪，用于精细化数据收集和协同贡献分数 (ΔSC) 计算。

### 0.1 核心目标：行为数据收集与量化 [x]

本阶段的核心任务是为 Qwen3-1.7B 模型中**每一个块（block）**计算其在 SFT 数据集上的行为摘要。

### 0.2 数据集：作为“广谱显影剂”的角色 [x]

我们选择的数据集（如大规模的 Claude 蒸馏集）扮演着一个 **“广谱显影剂”**的角色。其价值在于**广度**和**多样性**，通过将模型暴露在足够多的主题、任务和语言风格下，确保其内部几乎所有的“概念细胞”都有机会被激发和“点亮”。我们利用这个过程来观察和记录每个参数在各种刺激下的反应，从而为后续的“神经显影”提供最全面的行为数据，关心的是**覆盖率**，而非单个样本的极致难度。

### 0.3 硬件与精度策略 [x]

- **目标硬件**: 消费级设备（8GB VRAM, 32GB RAM）。
- **核心策略**:
  1. **全程 INT4 精度**: 模型加载、前向传播、反向传播均在 INT4 量化模式下进行，从根本上解决 GPU 显存瓶颈。
  2. **分析粒度**: 分析将深入到**块级（Block-level）**，即`bitsandbytes`的`blocksize=64`所定义的参数块。
  3. **低精度统计**: 不追求统计值的绝对精度，而是其**相对序数关系**。所有统计数据将使用低精度（如打包到两个字节的 INT4/INT2 值）进行存储。
  4. **内存数据库与持久化**: 使用内存中的 `SQLite` 数据库（约 4-8 GB）来存储全部参数的统计摘要，并**异步**将数据库备份到磁盘，以实现高性能、持久化和可恢复性。

### 0.4 模型与数据准备 [x]

- **模型加载**: 使用 `transformers` 和 `bitsandbytes` 库，加载 `Qwen/Qwen3-1.7B` 模型，必须启用 INT4 量化配置（`load_in_4bit=True`, `bnb_4bit_quant_type="nf4"`）。
- **SFT 数据集**: 选择高质量、多样化的 SFT 数据集，如 `HuggingFaceH4/ultrachat_200k`。对数据集进行预处理，包括使用 Qwen 的聊天模板进行格式化、分词，并准备 `input_ids`, `attention_mask` 和 `labels`。
- **数据加载器**: 创建 `DataLoader`，**`batch_size` 必须设置为 1**，以确保对每个样本的贡献进行独立分析。

### 0.5 精细化数据收集 [x]

目标是为 Qwen3-1.7B 模型中**每一个块（block）**计算其在 SFT 数据集上的行为摘要。

**已实现**: 已通过猴子补丁 `Linear4bit.forward` 和 `backward_hook` 机制，成功捕获并在线聚合了块级激活、权重 `absmax` 和梯度范数信息，并将其量化后存储到内存数据库中。数据收集流程已健壮化，能够处理各种模块类型并进行可视化。

### 0.6 协同贡献分数 (ΔSC) 计算 [x]

- **目标**: 根据收集到的聚合数据，为**每个块**计算一个量化的协同贡献分数 `ΔSC`。
- **核心原则**: **高激活 + 低梯度(高稳定性) + 高频次 = 高协同贡献**。
- **已实现**: 已根据激活 Z-score 和梯度范数 Z-score 计算 `ΔSC`，并已集成到可视化系统中。

## 阶段 1: 架构攻关：碎片化专家与动态卸载 (FEDO)

此阶段的核心目标是攻克 Tiny-ONN 的底层工程难题，实现一个能够从头开始训练、由海量微小专家组成的、并能在消费级硬件上运行的稀疏模型架构。

### 1.1 核心目标：建造，而非解剖

本阶段的设计哲学发生根本性转变：我们不再试图去“解剖”和重组一个庞大的、预训练好的稠密模型。相反，我们直接**设计并建造**一个从底层就符合 ONN 哲学的、原生稀疏的系统。我们将一个不可能完成的“神经外科手术”任务，转化为一个**虽然困难但可以被具体攻克的系统工程任务**。

### 1.2 关键技术突破点：FEDO 架构

**FEDO (Fragmented Experts & Dynamic Offloading)** 架构是本阶段的核心。

- **碎片化专家 (Fragmented Experts)**: 模型由海量（例如 1024 个或更多）的、极度微小（例如 8M-16M 参数）的专家模块组成。
- **动态卸载 (Dynamic Offloading)**: 这是实现大规模稀疏模型在有限显存上训练的关键。
  - **权重卸载**: 在任何时刻，仅有当前批次计算所需的、被路由系统激活的少数专家权重（`top-k`）驻留在 GPU 显存中。绝大多数（99% 以上）未被激活的专家权重，将被**卸载 (Offload)** 到 CPU 内存，甚至更低成本的 NVMe SSD 存储中。
  - **异步预取**: 在路由网络计算出下一批次可能需要的专家后，系统将**异步地、提前地 (Prefetch)** 将这些专家的权重从 CPU/SSD 加载回 GPU 显存，以掩盖 I/O 延迟，最小化计算等待时间。

### 1.3 工程实现路径

- **深入 PyTorch 底层**: 投入主要精力研究 PyTorch 的底层机制，特别是 `torch.distributed` 的通信原语、自定义显存管理和 CUDA 流。
- **实现自定义显存管理器**: 编写一个能够接管部分权重张量（`torch.Tensor`）的显存分配和释放逻辑的管理器，实现与 CPU/SSD 之间的高效、异步数据交换。
- **构建 ONN 原型**: 在 FEDO 架构之上，从头开始构建并实现一个功能完备的、小型的 ONN 原型。

## 阶段 2: 永续预训练与能力涌现

此阶段的目标是在 FEDO 架构之上，对 ONN 原型进行大规模的、持续的预训练，并观察和引导其复杂能力的涌现。

### 2.1 训练策略：SMK 与路由协同进化

- **灾难性遗忘的降维**: 正如理论预期，稀疏 MoE 架构将“知识遗忘”的核心难题，降维到了“路由遗忘”。大部分专家权重在单次更新中是冻结的，知识得以保存。
- **路由遗忘的修复**: 针对路由网络可能产生的遗忘，采用**小规模样本重放 (Experience Replay)** 的策略，定期用少量旧任务数据“唤醒”和“校准”路由器的记忆。
- **SMK 策略**: 继续采用 SMK（Surprise Min_K）策略进行梯度更新，保护和强化已形成的稳定专家知识，同时鼓励新知识的探索。

### 2.2 核心目标：实现真正的持续学习

- **永续训练**: 设计并实现一个可以 7x24 小时不间断进行预训练的流程。模型将持续不断地从数据流中学习新知识、新技能。
- **能力涌现**: 使用`ΔSC` & `ΣPI` & `ΩID` 工具箱全程监控训练过程，观察专家功能是如何自发分化的，知识共享是如何在专家之间涌现的，以及整体模型能力是如何随训练而增长的。
- **即插即用**: 探索实现“热插拔”专家模块的可能性。即在模型持续运行的情况下，动态地加入新的、未经训练的专家模块，或移除、归档旧的、不常用的专家模块。
