# 开发日志：SMK² (SC Max-K) 策略

**版本:** 1.0
**日期:** 2025-07-16
**状态:** 概念设计与规划

## 1. 背景与动机

在 Tiny-ONN 的持续预训练阶段，我们采用稀疏混合专家（SMoE）架构，旨在通过专家分化和自适应路由实现高效的知识调度。此前的 PILF 框架中，我们探索了 **Surprise Min-K (SMK)** 策略，该策略通过仅更新梯度范数（Surprise）最小的 `min_k` 个专家来诱导功能分化。SMK 的核心思想是优先巩固模型“最不惊讶”的知识，即那些已经掌握得较好的领域。

然而，单纯的梯度范数（Surprise）作为专家更新的唯一指标存在局限性。一个专家即使梯度范数很低，可能仅仅因为它在当前任务中没有被充分激活，或者其贡献度本身就不高。我们追求的专家，不仅要“稳定”，更要“高效”和“关键”。

随着我们对 `∫ΔSC` (Synergistic Contribution) 理论的深入理解和 `scanner` 工具的成功开发，我们现在拥有了一个更全面、更精确的专家贡献度量指标。`ΔSC` 结合了激活值（衡量使用程度）和梯度范数（衡量稳定性/惊奇度），能够量化一个参数块在特定任务中**高效协同贡献**的程度。

因此，我们提出并设计 **SMK² (ΔSC Max-K)** 策略，旨在利用 `ΔSC` 的优势，实现更精准、更高效的专家功能分化和共享专家涌现。

## 2. 核心概念：ΔSC Max-K (SMK²)

SMK² 策略的核心在于，在每次训练迭代中，我们不再根据梯度范数选择 `min_k` 个专家进行更新，而是根据**ΔSC 分数选择 `max_k` 个专家进行梯度更新**。

### 2.1 ΔSC 作为专家更新指标的优势

`ΔSC` 的定义为 `z_score(Activation) - z_score(Gradient Norm)`。它同时考虑了专家的“使用程度”和“稳定性”，从而提供了比单一梯度范数更优越的更新准则：

- **高激活 (High Activation):** 确保被更新的专家是当前计算通路中的活跃参与者，而非边缘或不相关的模块。
- **低梯度范数 (Low Gradient Norm):** 确保被更新的专家在处理当前任务时表现出高度的确定性和稳定性，即其内部预测与实际目标高度一致。
- **高效贡献的量化:** `ΔSC` 分数高的专家，意味着它在当前任务中既被频繁使用，又以一种“胸有成竹”的方式进行处理。这些专家是模型中真正的“资深专家”或“核心贡献者”。通过优先更新这些专家，我们能够更有效地强化模型的核心能力。

### 2.2 与旧版 SMK 的对比

| 特性/策略          | 旧版 SMK (Surprise Min-K)              | 新版 SMK² (ΔSC Max-K)                      |
| :----------------- | :------------------------------------- | :----------------------------------------- |
| **更新指标**       | 梯度范数 (Surprise)                    | ΔSC (Synergistic Contribution)             |
| **选择逻辑**       | 选择梯度范数**最小**的 `min_k` 个专家  | 选择 ΔSC 分数**最高**的 `max_k` 个专家     |
| **侧重目标**       | 优先巩固“最不惊讶”的知识               | 优先强化“最有效且稳定贡献”的知识           |
| **潜在局限**       | 可能忽略那些虽有“惊讶”但贡献关键的专家 | -                                          |
| **与 IPWT 契合度** | 间接                                   | **直接**，ΔSC 本身就是 IPWT 核心概念的量化 |

## 3. 技术实现细节

SMK² 策略的实现将集成到 Tiny-ONN 的持续预训练流程中，特别是 `SurpriseMinKStrategy` 模块（或其更名后的 `DeltaSCMaxKStrategy`）。

1. **ΔSC 分数计算：** 在每次前向传播和反向传播后，我们已经能够为每个参数块（以及聚合后的专家）计算出其 `ΔSC` 分数。这一过程是 `scanner` 的核心功能，且几乎没有额外计算成本。
2. **专家选择：**
   - 在计算完所有专家的 `ΔSC` 分数后，我们将使用 `torch.topk(delta_sc_scores, k=max_k, largest=True)` 来选择 `max_k` 个 `ΔSC` 分数最高的专家。
   - 这些被选中的专家，其梯度将被保留并用于优化器更新。其余未被选中的专家的梯度将被置零。
3. **动态 `max_k` (未来扩展):** 尽管初始实现可能使用固定的 `max_k` 值，但未来可以探索根据模型的预测完整性（PI）或其他实时指标，动态调整 `max_k` 的数量，以实现更精细的自适应学习。
4. **与门控网络的协同：** SMK² 策略将与我们基于高斯分布的门控网络协同进化。门控网络学习如何路由，而 SMK² 则指导专家如何分化和强化。

## 4. 预期效益

SMK² 策略的引入，预计将为 Tiny-ONN 带来以下显著效益：

- **更高效的专家功能分化：** 通过优先更新那些贡献度最高且最稳定的专家，模型将更快地形成清晰、专业的知识分区。
- **自然涌现的共享专家：** 那些在多种任务和不同情境下，其 `ΔSC` 分数都持续名列前茅的专家，将自然而然地演化为模型中的“共享专家”，无需手动指定。这比现有 MoE 模型（如 DeepSeek-V3 或 Kimi K2）手动指定共享专家数量的设计更为先进和自适应。
- **提升训练稳定性和收敛速度：** 专注于更新最有效的参数，可以减少无效或有害的梯度传播，从而提高整体训练过程的稳定性和收敛效率。
- **更强的可解释性：** 最终形成的专家将具有更清晰的功能边界和更高的 `ΔSC` 分数，从而更容易进行语义标注和可解释性分析。
- **理论与实践的统一：** SMK² 策略直接将 `ΔSC` 这个 IPWT 核心概念融入到模型训练的循环中，进一步强化了 Tiny-ONN 的理论基础和实践效果。

## 5. 挑战与未来工作

- **`max_k` 的超参数调优：** 需要通过实验确定不同模型规模和任务下的最佳 `max_k` 值。
- **实验验证：** 在 Phase 2 中，需要对 SMK² 策略进行严格的实验验证，包括与旧版 SMK 的性能对比、专家分化效果的量化分析等。
- **动态 `max_k` 机制的开发：** 探索将 `max_k` 与模型实时状态（如 PI）动态关联的算法。
- **与门控网络学习的交互：** 深入研究 SMK² 对门控网络学习行为的影响，确保两者协同优化。

通过 SMK² 策略，我们相信能够将 Tiny-ONN 的“知识提取”和“持续预训练”提升到一个全新的水平，使其成为一个真正高效、可解释且自适应的稀疏智能系统。
