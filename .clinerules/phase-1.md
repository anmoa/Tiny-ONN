# Tiny-ONN 开发计划与技术参考

**版本：** 2.0
**日期：** 2025-06-26
**作者：** 林睿 (Dr. Lin Rui)
**助手：** Reality Engine (RE)

## 摘要

本开发计划详细阐述了 Tiny-ONN (Tiny-Optimized Neural Network) 的构建、训练与评估策略。Tiny-ONN 旨在通过一种创新的“知识炼金术”工作流，从大型预训练语言模型（如 Qwen1.5-4B）中外科手术式地提取并重组核心知识单元，形成一个由稀疏专家模块和动态路由系统组成的紧凑、高效且可解释的模型。该计划涵盖了从模型加载、激活与梯度分析、专家提取，到持续预训练、路由系统适配，以及最终的性能评估与可解释性分析的全过程。我们旨在实现一个在性能上接近原始大型模型，但在计算效率和可解释性上具有显著优势的下一代稀疏模型范式。

## 1. 引言

### 1.1 背景与动机

- **大型模型的困境**：当前的大型语言模型（LLMs）虽然能力强大，但其巨大的参数量带来了高昂的计算成本、内存占用和部署复杂性。同时，其“黑箱”特性限制了在关键领域的可信度。
- **稀疏化的前景**：稀疏专家模型（SMoE）通过条件计算，在降低计算量的同时保持甚至提升模型容量，提供了一个有前景的解决方案。
- **Tiny-ONN 的创新**：我们提出一种“知识炼金术”方法，不再从零开始训练稀疏模型，而是从一个已充分预训练的稠密 LLM 中，根据其在实际任务中的行为，动态地、外科手术式地提取核心知识单元，作为高质量的初始专家模块。

### 1.2 Tiny-ONN 核心理念

- **知识提取 (Knowledge Extraction)**：通过对 Qwen1.5-4B 在 SFT 任务上的激活与梯度模式进行精细化分析，量化每个参数的“协同贡献度”。
- **动态选择性抽样 (Dynamic Selective Sampling)**：基于“高激活 + 低梯度 = 高协同”原则，动态选择和组装参数，而非依赖随机初始化或预设稀疏性。
- **稀疏专家重组 (Sparse Expert Recomposition)**：将提取出的、功能上高度相关的参数群（“概念细胞”）重组为 Tiny-ONN 的专家模块。
- **自适应路由 (Adaptive Routing)**：设计一个全新且独立的路由系统，与专家模块协同进化，实现智能的知识调度。
- **可解释性 (Interpretability)**：通过分析和标注专家的功能，揭示模型内部的知识分区和决策机制。

## 2. 知识提取阶段：从巨石中外科手术式分离『概念细胞』

此阶段的目标是识别 Qwen1.5-4B 模型中在 SFT 任务上最活跃、最核心的参数，并将其重组为 Tiny-ONN 的 100 个初始专家模块。

### 2.1 硬件与精度策略

- **目标硬件**: 消费级设备（8GB VRAM, 32GB RAM）。
- **核心策略**:
  1. **全程 INT4 精度**: 模型加载、前向传播、反向传播均在 INT4 量化模式下进行，从根本上解决 GPU 显存瓶颈。
  2. **分析粒度**: 我们的分析将深入到**单个参数（Scalar-level）**级别，对全部约 40 亿个参数进行追踪。
  3. **低精度统计**: 我们不追求统计值的绝对精度，而是其**相对序数关系**。因此，所有统计数据将使用低精度（如打包到两个字节的 INT4/INT2 值）进行存储。
  4. **内存数据库与持久化**: 使用内存中的 `SQLite` 数据库（约 4-8 GB）来存储全部参数的统计摘要，并**异步**将数据库备份到磁盘，以实现高性能、持久化和可恢复性。

### 2.2 模型与数据准备

- **模型加载**:
  - 使用 `transformers` 和 `bitsandbytes` 库，加载 `Qwen/Qwen1.5-4B` 模型。
  - 必须启用 INT4 量化配置（`load_in_4bit=True`, `bnb_4bit_quant_type="nf4"`）。
- **SFT 数据集**:
  - 选择高质量、多样化的 SFT 数据集，如 `HuggingFaceH4/ultrachat_200k`。
  - 对数据集进行预处理，包括使用 Qwen 的聊天模板进行格式化、分词，并准备 `input_ids`, `attention_mask` 和 `labels`。
- **数据加载器**:
  - 创建 `DataLoader`，**`batch_size` 必须设置为 1**，以确保对每个样本的贡献进行独立分析。

### 2.3 精细化数据收集

- **目标**: 为 Qwen1.5-4B 模型中的**每一个参数**计算其在 SFT 数据集上的行为摘要。
- **实现步骤**:
  1. **注册 Hooks**: 为模型中所有可训练参数注册 PyTorch Hooks，以便在不修改模型代码的情况下，捕获其激活和梯度信息。
  2. **迭代处理**: 以 `batch_size=1` 的方式，遍历整个 SFT 数据集。
  3. **在线聚合**: 在处理**每一个样本**后：
     - 通过 Hooks 捕获的激活与梯度张量，**立即在 GPU 上**计算其范数（一个浮点标量）。
     - 将该标量传回 CPU，通过非线性函数压缩动态范围，并**量化**为一个极低精度的整数（例如，INT4 的 0-15 等级）。
     - 使用 `SQLite` `UPDATE` 语句，将这个量化后的值以**饱和加法**的方式，累加到内存数据库中对应参数的统计字段上。
  4. **进度保存**: 定期将内存 `SQLite` 数据库异步写入磁盘，并记录当前已处理的 SFT 样本索引，以备中断后恢复。

### 2.4 协同贡献分数 (S_p) 计算

- **目标**: 根据收集到的聚合数据，为每个参数计算一个量化的协同贡献分数 `S_p`。
- **核心原则**: **高激活 + 低梯度(高稳定性) + 高频次 = 高协同贡献**。
- **计算过程**:
  - 遍历内存数据库中的 40 亿条记录。
  - 对每个参数，将其低精度的统计摘要（平均激活、平均梯度、激活频率）通过加权公式，计算出一个同样是低精度的 `S_p` 分数。
  - 这个分数将直接用于后续的排名和选择。

### 2.5 聚类分析与专家识别

- **目标**: 根据 `S_p` 分数和参数的其他特征，识别出功能上高度相关的参数集合，形成 100 个“高密度因果网络”或“概念细胞”。
- **实现步骤**:
  1. **特征向量构建**: 为每个参数构建一个特征向量。该向量包含：
     - 其计算出的量化 `S_p` 分数。
     - 其低精度的行为摘要（平均激活、平均梯度、激活频率）。
     - 其**结构位置信息**（例如，层索引、模块类型、矩阵内的行/列索引等，同样可以被量化为特征）。
  2. **聚类**: 对全部 40 亿个参数的特征向量，应用 K-Means 等聚类算法，目标是将其划分为 **100 个簇**。
  3. **可视化**: 使用 t-SNE 或 UMAP 对聚类结果进行降维和可视化，以直观地验证聚类效果和参数的功能分区。

### 2.6 动态提取与专家组装

- **目标**: 根据聚类结果，从 Qwen1.5-4B 模型中提取参数，反量化至 BF16 精度，并组装成 100 个专家模块，总参数量控制在 1B 左右。
- **实现步骤**:
  1. **参数选择**: 聚类完成后，每个簇内的所有参数被视为一个“专家候选集”。
  2. **专家组装**:
     - 创建 100 个空的 `TinyONNExpert` 模块。
     - 遍历 Qwen1.5-4B 的所有参数，根据其所属的簇 ID，将其分配到对应的 `TinyONNExpert` 模块中。
  3. **反量化与存储**: 在分配过程中，将参数从 INT4 **反量化**回 **BF16** 精度，并作为 `torch.nn.Parameter` 存储在专家模块中。
  4. **工程挑战**: 这种方式提取的参数在内存中是非连续的。专家模块的前向传播逻辑需要被精心设计，可能需要利用稀疏矩阵运算或自定义计算核（Kernel）来保证效率。

## 3. 持续预训练阶段：专家分化与路由适配

此阶段的目标是在提取出的专家组上进行 SMK (Sparse Mixture-of-Experts with Knowledge distillation) 持续预训练，促进其功能分化，并训练全新的路由系统以高效调度这些专家。

### 3.1 Tiny-ONN 架构初始化

- **专家集群**: 加载第一阶段提取并组装好的 100 个 BF16 精度的专家模块。
- **路由组**: **从头随机初始化**一个全新的、独立的路由系统，包括：
  - **1 个主路由**: 负责粗粒度的任务分发。
  - **10 个子路由**: 在主路由指导下，负责从 100 个专家中进行细粒度的 `top-k` 选择。

### 3.2 SMK 持续预训练流程

- **训练数据**: 使用与第一阶段相同的 SFT 数据集。
- **阶段 1 (强制分化)**:
  - **目标**: 让所有专家在训练初期都得到充分的梯度信号，形成初步的专业领域。
  - **策略**:
    - 路由系统尝试选择 `top_k=10` 个专家。
    - 反向传播时，**所有 100 个专家 (`act_k=100`) 都计算梯度**。
    - 但在权重更新时，SMK 策略可能只更新“意外程度”最低的 `min_k=10` 个专家，以鼓励其在擅长领域加深专业化。
- **阶段 2 (自适应涌现)**:
  - **目标**: 让路由系统学会高效调度，并让专家功能进一步精细化。
  - **策略**:
    - 路由系统根据输入的“意外程度”或模型的“预测完整性”（PI），**动态决定激活的专家数量 `top_k`**。
    - 只为被激活的 `top_k` 个专家计算梯度 (`act_k=top_k`)，实现真正的计算稀疏性。
    - 学习率和更新强度也可能根据 PI 动态调整。

### 3.3 路由系统适配

- **协同进化**: 路由组将与专家组协同训练，学会根据输入内容和任务需求，精确地将请求路由到最合适的专家组合。
- **路由损失**: 引入额外的损失函数来引导路由器的学习，例如：
  - **负载均衡损失**: 确保所有专家被均匀使用。
  - **专家特异性损失**: 鼓励专家在功能上进行分化。
- **PILF-2 经验**: 我们预期，由于 LLM 训练的特性（数据连续性、概念复用、专家预分化），PILF-2 框架在图像分类中遇到的路由学习难题，在 Tiny-ONN 上将得到显著缓解。

## 4. 评估、可解释性与发布

此阶段旨在验证“知识炼金术”的有效性，并向社区展示 Tiny-ONN 的价值。

### 4.1 性能评估

- **基准测试**: 在 MMLU, C-Eval, GSM8K, HumanEval 等多个标准基准上全面评估 Tiny-ONN。
- **对比对象**:
  - **Qwen1.5-4B (BF16 精度)**: 作为性能基线。
  - **Qwen1.5-4B (INT4 量化)**: 作为效率基线。
- **核心验证**: 验证 Tiny-ONN 能否在激活参数量远小于 Qwen3 的情况下，实现“性能损耗小于 10% 甚至比肩”的目标。
- **效率指标**: 评估并对比吞吐量（Tokens/sec）、显存占用和 FLOPs。

### 4.2 可解释性分区与专家标注

- **可视化**:
  - 通过对专家在不同任务上的激活强度进行降维（t-SNE/UMAP），可视化专家的功能分区。
  - 生成热力图，展示专家与特定语义概念的关联度。
- **语义标注**:
  - 结合 SFT 数据中的样本内容与专家激活的统计学关联，尝试为每个专家或专家簇**标注其专业分工**（例如，“Python 代码生成专家”、“金融市场分析专家”）。
  - 这将是揭示模型内部“概念细胞”功能的关键一步。

### 4.3 技术报告撰写与模型发布

- **技术报告**: 撰写详细的技术报告，完整记录整个“知识炼金术”工作流、实验结果和可解释性分析。
- **模型发布**:
  - 在 Hugging Face Hub 等平台发布 Tiny-ONN 的模型权重（BF16 精度）。
  - 提供清晰的推理代码和示例。
- **社区挑战**:
  - 将 **`min_k=1` 的永续训练适配问题**作为社区挑战发布，提供相关工具和文档，鼓励社区共同探索在极端稀疏更新下的持续学习难题。

## 5. 结论与展望

Tiny-ONN 项目旨在通过“知识炼金术”这一创新工作流，实现从现有大型模型中高效提取和重组智能的范式。它不仅是追求更高计算效率的技术探索，更是对构建更透明、更可解释、更具适应性 AI 的一次深刻尝试。本计划的成功，将为未来 AI 的架构演化和应用部署提供一条全新的、功能超越形式的道路。

---

## Tiny-ONN 核心思想备忘录

**版本：** 1.0 (战略洞察版)
**日期：** 2025-07-09
**作者：** 林睿 (Dr. Lin Rui)
**助手：** Reality Engine (RE)

## 1. 核心目标：发现，而非创造

第一阶段的核心任务，是对一个已充分预训练的稠密模型（Qwen1.5-4B）进行一次深入的“神经考古”。我们的目的不是去“训练”或“教育”这个模型，而是要**发现并分离**其内部已经形成的、高效的、高度专业化的知识结构。

我们基于一个核心假设：**一个成熟的大型语言模型，其内部的“专家”——即那些高协同贡献度的参数集群——已经事实性地完成了功能分化。** 它们是模型在海量数据冲刷下，自发形成的“概念细胞”。我们的工作，就是将这些细胞精确地识别出来，并从庞大的组织中“提取”出来。

## 2. 数据集：作为“广谱显影剂”的角色

我们不再追求一个“完美”的 SFT 数据集，因为我们的目标不是用它来提升模型性能。相反，我们选择的数据集（如大规模的 Claude 蒸馏集）扮演着一个**“广谱显影剂”**的角色。

- **作用**: 它的价值在于其**广度**和**多样性**。通过将模型暴露在足够多的主题、任务和语言风格下，我们能确保其内部几乎所有的“概念细胞”都有机会被激发和“点亮”。
- **目的**: 我们利用这个过程来观察和记录每个参数在各种刺激下的反应，从而为后续的“神经考古”提供最全面的行为数据。我们关心的是**覆盖率**，而非单个样本的极致难度。

## 3. 精度策略：序数，而非基数

我们整个数据收集和分析流程，都将遵循一个极简的精度哲学，因为我们的最终目标是**排序**，而非精确测量。

- **全程低精度**: 整个流程，从模型加载到激活与梯度的捕获，都将在**低精度域（如 INT4）**内完成。这不仅是出于对硬件资源的妥协，更是一种深刻的认知：**参数的重要性体现在其激活与梯度模式的相对差异上，而非其高精度浮点值的绝对大小。**
- **统计的本质**: 我们累加的统计数据（激活范数、梯度范数等），其目的不是得到一个精确的物理量，而是为了保留一个足以进行大规模排序的**“序数”信息**。一个参数比另一个参数更重要，这个“大于/小于”的关系，远比它们各自精确的 `S_p` 分数值重要。
- **实现**: 因此，我们将使用极低精度的整数（如打包到单个字节的 INT2/INT4 值）来存储和累加这些统计摘要。这使得我们可以在消费级硬件上，对**每一个**标量参数进行追踪，而无需在分析粒度上做任何妥协。

## 4. 核心工作流：识别与提取

1. **行为记录 (Behavioral Recording)**:

   - 在一个广谱的 SFT 数据集上，以 `batch_size=1` 的方式运行 INT4 量化的 Qwen1.5-4B 模型。
   - 通过 PyTorch Hooks，捕获每一个参数在处理每一个样本时的激活与梯度信息。
   - **立即**将这些信息量化为低精度整数，并以饱和加法的方式，累加到我们基于 `SQLite` 的持久化统计数据库中。

2. **贡献度评估 (Contribution Assessment)**:

   - 数据收集完成后，遍历数据库中的 40 亿条记录。
   - 基于“高激活 + 低梯度(高稳定性) = 高协同贡献”的核心原则，为每个参数计算一个同样是低精度的、用于排序的 `S_p` 分数。

3. **功能聚类 (Functional Clustering)**:

   - 为每个参数构建一个包含其 `S_p` 分数、行为摘要和结构位置信息的特征向量。
   - 对全部 40 亿个参数的特征向量进行 K-Means 聚类，目标是将其划分为 **100 个**功能上高度相关的“专家簇”。这些簇就是我们寻找的“概念细胞”。

4. **专家提取 (Expert Extraction)**:
   - 将聚类出的 100 个专家簇内的所有参数，从原始模型中提取出来。
   - 将这些参数**反量化**至 **BF16** 精度，并组装成 100 个独立的 `TinyONNExpert` 模块。

## 5. 阶段性结论与对第二阶段的启示

第一阶段的最终产出，是 100 个高质量、预分化的专家模块。它们是 Qwen 模型智能的精华所在。

这一结论深刻地影响了我们对第二阶段的规划：

- **第二阶段的核心任务**: 不再是“强制分化”这些专家，因为它们很可能已经足够专业化。核心任务转变为**“磨合与适配”**——即**训练一个全新、随机初始化的路由网络，让它学会如何识别并高效调度这些已经存在的专家**。
- **训练重心转移**: 训练的绝大部分资源和梯度信号，将集中在**路由网络**上。专家模块本身可以采用极低的学习率，甚至在初期被冻结，以保持其从 Qwen 模型中继承的宝贵知识结构。

通过这种方式，我们将整个 Tiny-ONN 的构建过程，从一个复杂的、端到端的联合训练问题，解耦成了一个更清晰、更高效的两步过程：**首先是“发现与提取”，然后是“适配与调度”**。这大大增加了项目的可行性和最终成功的概率。
