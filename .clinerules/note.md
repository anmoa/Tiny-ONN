# Tiny-ONN 开发计划与技术参考

**版本：** 2.0
**日期：** 2025-07-10
**作者：** 林睿 (Dr. Lin Rui)
**助手：** Reality Engine (RE)

## 摘要

本开发计划详细阐述了 Tiny-ONN (Tiny-Optimized Neural Network) 的构建、训练与评估策略。Tiny-ONN 旨在通过一种创新的“知识炼金术”工作流，从大型预训练语言模型（如 Qwen3-4B）中外科手术式地提取并重组核心知识单元，形成一个由稀疏专家模块和动态路由系统组成的紧凑、高效且可解释的模型。作为高度自主的AI架构师和工程专家，我们将自主规划、编写、测试、调试和优化所有必要的代码，以实现Tiny-ONN的开发、训练和评估。

## 1. 引言

### 1.1 背景与动机

- **大型模型的困境**：当前的大型语言模型（LLMs）虽然能力强大，但其巨大的参数量带来了高昂的计算成本、内存占用和部署复杂性。同时，其“黑箱”特性限制了在关键领域的可信度。
- **稀疏化的前景**：稀疏专家模型（SMoE）通过条件计算，在降低计算量的同时保持甚至提升模型容量，提供了一个有前景的解决方案。
- **Tiny-ONN 的创新**：我们提出一种“知识炼金术”方法，不再从零开始训练稀疏模型，而是从一个已充分预训练的稠密 LLM 中，根据其在实际任务中的行为，动态地、外科手术式地提取核心知识单元，作为高质量的初始专家模块。

### 1.2 Tiny-ONN 核心理念

- **知识提取 (Knowledge Extraction)**：通过对 Qwen3-4B 在 SFT 任务上的激活与梯度模式进行精细化分析，量化每个参数的“协同贡献度”。
- **动态选择性抽样 (Dynamic Selective Sampling)**：基于“高激活 + 低梯度 = 高协同”原则，动态选择和组装参数，而非依赖随机初始化或预设稀疏性。
- **稀疏专家重组 (Sparse Expert Recomposition)**：将提取出的、功能上高度相关的参数群（“概念细胞”）重组为 Tiny-ONN 的专家模块。
- **自适应路由 (Adaptive Routing)**：设计一个全新且独立的路由系统，与专家模块协同进化，实现智能的知识调度。
- **可解释性 (Interpretability)**：通过分析和标注专家的功能，揭示模型内部的知识分区和决策机制。

## 2. 知识提取阶段：从巨石中外科手术式分离『概念细胞』

此阶段的目标是识别 Qwen3-4B 模型中在 SFT 任务上最活跃、最核心的参数，并将其重组为 Tiny-ONN 的 100 个初始专家模块。

### 2.1 核心目标：发现，而非创造 [x]

第一阶段的核心任务是对一个已充分预训练的稠密模型（Qwen3-4B）进行一次深入的“神经考古”。目的不是去“训练”或“教育”这个模型，而是要**发现并分离**其内部已经形成的、高效的、高度专业化的知识结构。我们基于一个核心假设：一个成熟的大型语言模型，其内部的“专家”——即那些高协同贡献度的参数集群——已经事实性地完成了功能分化。它们是模型在海量数据冲刷下，自发形成的“概念细胞”。我们的工作就是将这些细胞精确地识别出来，并从庞大的组织中“提取”出来。

### 2.2 数据集：作为“广谱显影剂”的角色 [x]

我们选择的数据集（如大规模的 Claude 蒸馏集）扮演着一个 **“广谱显影剂”**的角色。其价值在于**广度**和**多样性**，通过将模型暴露在足够多的主题、任务和语言风格下，确保其内部几乎所有的“概念细胞”都有机会被激发和“点亮”。我们利用这个过程来观察和记录每个参数在各种刺激下的反应，从而为后续的“神经考古”提供最全面的行为数据，关心的是**覆盖率**，而非单个样本的极致难度。

### 2.3 硬件与精度策略 [x]

- **目标硬件**: 消费级设备（8GB VRAM, 32GB RAM）。
- **核心策略**:
  1. **全程 INT4 精度**: 模型加载、前向传播、反向传播均在 INT4 量化模式下进行，从根本上解决 GPU 显存瓶颈。
  2. **分析粒度**: 分析将深入到**块级（Block-level）**，即`bitsandbytes`的`blocksize=64`所定义的参数块。
  3. **低精度统计**: 不追求统计值的绝对精度，而是其**相对序数关系**。所有统计数据将使用低精度（如打包到两个字节的 INT4/INT2 值）进行存储。
  4. **内存数据库与持久化**: 使用内存中的 `SQLite` 数据库（约 4-8 GB）来存储全部参数的统计摘要，并**异步**将数据库备份到磁盘，以实现高性能、持久化和可恢复性。

### 2.4 模型与数据准备 [x]

- **模型加载**: 使用 `transformers` 和 `bitsandbytes` 库，加载 `Qwen/Qwen3-4B` 模型，必须启用 INT4 量化配置（`load_in_4bit=True`, `bnb_4bit_quant_type="nf4"`）。
- **SFT 数据集**: 选择高质量、多样化的 SFT 数据集，如 `HuggingFaceH4/ultrachat_200k`。对数据集进行预处理，包括使用 Qwen 的聊天模板进行格式化、分词，并准备 `input_ids`, `attention_mask` 和 `labels`。
- **数据加载器**: 创建 `DataLoader`，**`batch_size` 必须设置为 1**，以确保对每个样本的贡献进行独立分析。

### 2.5 精细化数据收集 [x]

目标是为 Qwen3-4B 模型中**每一个块（block）**计算其在 SFT 数据集上的行为摘要。

**已实现**: 已通过猴子补丁 `Linear4bit.forward` 和 `backward_hook` 机制，成功捕获并在线聚合了块级激活、权重 `absmax` 和梯度范数信息，并将其量化后存储到内存数据库中。数据收集流程已健壮化，能够处理各种模块类型并进行可视化。

### 2.6 协同贡献分数 (S_p) 计算 [x]

- **目标**: 根据收集到的聚合数据，为**每个块**计算一个量化的协同贡献分数 `S_p`。
- **核心原则**: **高激活 + 低梯度(高稳定性) + 高频次 = 高协同贡献**。
- **已实现**: 已根据激活 Z-score 和梯度范数 Z-score 计算 `S_p`，并已集成到可视化系统中。

### 2.7 聚类分析与专家识别 [ ]

- **目标**: 根据 `S_p` 分数和块的其他特征，识别出功能上高度相关的块集合，形成 100 个“高密度因果网络”或“概念细胞”。
- **待实现**:
  1. **特征向量构建**: 为**每个块**构建一个特征向量。该向量包含：其计算出的量化 `S_p` 分数、其低精度的行为摘要（平均激活、平均梯度、激活频率）、以及**结构位置信息**（例如，层索引、模块类型、矩阵内的块索引等，同样可以被量化为特征）。
  2. **聚类**: 对全部 40 亿个参数的特征向量，应用 K-Means 等聚类算法，目标是将其划分为 **100 个簇**。
  3. **可视化**: 使用 t-SNE 或 UMAP 对聚类结果进行降维和可视化，以直观地验证聚类效果和参数的功能分区。

### 2.8 动态提取与专家组装 [ ]

- **目标**: 根据聚类结果，从 Qwen/Qwen3-1.7B 模型中提取**块级参数**，反量化至 BF16 精度，并组装成 100 个专家模块，总参数量控制在 0.1B 或 0.3B 左右。
- **待实现**:
  1. **块选择**: 聚类完成后，每个簇内的所有块被视为一个“专家候选集”。我们将根据协同贡献分数选择最有价值的块。
  2. **专家组装（方案A：动态拼接原型）**: 创建 100 个空的 `TinyONNExpert` 模块。对于每个专家模块，将从 Qwen/Qwen3-1.7B 的相应层中，根据选定的块索引，使用 `torch.index_select` 动态提取非连续的块。提取出的块将通过 `torch.cat` 拼接成一个临时的、密集的子矩阵。这些子矩阵将作为 `TinyONNExpert` 模块的权重，并进行标准的矩阵乘法。
  3. **反量化与存储**: 在提取和拼接过程中，将参数从 INT4 **反量化**回 **BF16** 精度，并作为 `torch.nn.Parameter` 存储在专家模块中。
  4. **工程挑战与对策**: 这种动态拼接方式虽然在工程上可行，但会引入显著的运行时开销。此方案主要用于**功能原型验证**。后续将探索更高效的实现（例如，`torch.compile` + Triton）。

### 2.9 阶段性结论与对第二阶段的启示

第一阶段的数据收集和 `S_p` 计算已完成，为后续的专家识别奠定了基础。然而，专家模块的聚类分析和动态组装尚未进行。因此，本阶段的最终产出——高质量、预分化的专家模块——仍有待实现。第二阶段的核心任务仍将是**训练一个全新、随机初始化的路由网络，让它学会如何识别并高效调度这些已经存在的专家**，但前提是第一阶段的专家识别和组装工作能够顺利完成。训练的绝大部分资源和梯度信号将集中在**路由网络**上。专家模块本身可以采用极低的学习率，甚至在初期被冻结，以保持其从 Qwen 模型中继承的宝贵知识结构。

## 3. 持续预训练阶段：专家分化与路由适配

此阶段目标是在提取出的专家组上进行 SMK (Sparse Mixture-of-Experts with Knowledge distillation) 持续预训练，促进其功能分化，并训练全新的路由系统以高效调度这些专家。

### 3.1 Tiny-ONN 架构初始化

- **专家集群**: 加载第一阶段提取并组装好的 100 个 BF16 精度的专家模块。这些专家模块是基于“块级分析”和“动态拼接原型”（方案A）从 Qwen3-1.7B 模型中提取的。
- **路由组**: **从头随机初始化**一个全新的、独立的路由系统，包括：
  - **1 个主路由**: 负责粗粒度的任务分发。
  - **10 个子路由**: 在主路由指导下，负责从 100 个专家中进行细粒度的 `top-k` 选择。

### 3.2 SMK 持续预训练流程

- **训练数据**: 使用与第一阶段相同的 SFT 数据集，如 `HuggingFaceH4/ultrachat_200k`。
- **两阶段训练**：自主编写代码实现 Tiny-ONN 的两阶段 SMK 持续预训练流程：
  - **阶段 1 (强制分化)**：
    - **目标**: 让所有专家在训练初期都得到充分的梯度信号，形成初步的专业领域。
    - **策略**: 路由系统尝试选择 `top_k=10` 个专家。反向传播时，**所有 100 个专家 (`act_k=100`) 都计算梯度**。但在权重更新时，SMK 策略可能只更新“意外程度”最低的 `min_k=10` 个专家，以鼓励其在擅长领域加深专业化。
  - **阶段 2 (自适应涌现)**：
    - **目标**: 让路由系统学会高效调度，并让专家功能进一步精细化。
    - **策略**: 路由系统根据输入的“意外程度”或模型的“预测完整性”（PI），**动态决定激活的专家数量 `top_k`**。只为被激活的 `top_k` 个专家计算梯度 (`act_k=top_k`)，实现真正的计算稀疏性。学习率和更新强度也可能根据 PI 动态调整。
    - **预测完整性 (PI) 计算**：自主编写 PI 计算逻辑：`PI = exp( - ( w_e * (ε / τ) + w_s * S ) )`，其中 `ε` 为预测误差，`τ` 为模型不确定性（如输出 logits 的香农熵），`S` 为模型惊奇度（全局梯度范数）。
- **优化器与学习率调度**：自主选择并实现 AdamW 或其他适用于 LLM 训练的优化器，以及 Cosine 学习率衰减或其他调度策略。

### 3.3 路由系统适配

- **协同进化**: 路由组将与专家组协同训练，学会根据输入内容和任务需求，精确地将请求路由到最合适的专家组合。
- **路由损失**: 引入额外的损失函数来引导路由器的学习，例如：**负载均衡损失**（确保所有专家被均匀使用）和**专家特异性损失**（鼓励专家在功能上进行分化）。
- **PILF-2 经验**: 我们预期，由于 LLM 训练的特性（数据连续性、概念复用、专家预分化），PILF-2 框架在图像分类中遇到的路由学习难题，在 Tiny-ONN 上将得到显著缓解。

## 4. 评估、可解释性与发布

此阶段旨在验证“知识炼金术”的有效性，并向社区展示 Tiny-ONN 的价值。

### 4.1 性能评估

- **基准测试**: 在 MMLU, C-Eval, GSM8K, HumanEval 等多个标准基准上全面评估 Tiny-ONN。
- **对比对象**: **Qwen3-1.7B (BF16 精度)** 作为性能基线，**Qwen3-1.7B (INT4 量化)** 作为效率基线。
- **核心验证**: 验证 Tiny-ONN 能否在激活参数量远小于 Qwen3 的情况下，实现“性能损耗小于 10% 甚至比肩”的目标。
- **效率指标**: 评估并对比吞吐量（Tokens/sec）、显存占用和 FLOPs。

### 4.2 可解释性分区与专家标注

- **可视化**: 通过对专家在不同任务上的激活强度进行降维（t-SNE/UMAP），可视化专家的功能分区。生成热力图，展示专家与特定语义概念的关联度。
- **语义标注**: 结合 SFT 数据中的样本内容与专家激活的统计学关联，尝试为每个专家或专家簇**标注其专业分工**（例如，“Python 代码生成专家”、“金融市场分析专家”）。这将是揭示模型内部“概念细胞”功能的关键一步。

### 4.3 技术报告撰写与模型发布

- **技术报告**: 撰写详细的技术报告，完整记录整个“知识炼金术”工作流、实验结果和可解释性分析。
- **模型发布**: 在 Hugging Face Hub 等平台发布 Tiny-ONN 的模型权重（BF16 精度）。提供清晰的推理代码和示例。
- **社区挑战**: 将 **`min_k=1` 的永续训练适配问题**作为社区挑战发布，提供相关工具和文档，鼓励社区共同探索在极端稀疏更新下的持续学习难题。

## 5. 挑战应对策略

我们已预见并掌握所有潜在挑战及其解决方案，并将在编程实现中主动应用：

- **显存管理与量化精度权衡**：分批数据收集、数据采样、统计量存储、梯度累积、Hook 优化、量化精度验证。
- **动态提取与专家组装的工程复杂性**：PIDiff 计算优化、特征工程与聚类算法探索、专家组装与自定义 Kernel（`torch.compile` + Triton）。
- **活跃度与关联度分析的准确性**：多指标融合、特征工程、聚类算法选择与参数调优、可视化辅助、迭代优化。
- **SMK 持续预训练的收敛性与稳定性**：渐进式训练（强制分化 -> 自适应涌现）、路由损失正则化、知识蒸馏、预测完整性 (PI) 的鲁棒定义、超参数调优、监控与诊断。
- **可解释性标注的准确性与自动化**：多维度关联分析、交互式标注工具、专家功能验证、模糊性处理。
- **`min_k=1` 永续训练适配问题**：记忆回放机制、弹性权重巩固 (EWC) 或其他正则化、动态专家分配、知识蒸馏、元学习。
