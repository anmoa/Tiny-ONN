# Tiny-ONN 0.6B Hyper-SMoE - 模型卡片与设计报告

**日期**: 2025-07-19

## 1. 核心理念

**Tiny-ONN 0.6B** 是一个探索性的、基于“超稀疏混合专家”（Hyper-SMoE）架构的语言模型。其核心设计哲学是“**动态激活量+海量超轻量专家**”，旨在验证通过先进的动态稀疏训练范式，能否在消费级硬件上高效地训练一个具备高性能的 MoE 模型。

与主流 MoE 模型（如 Mixtral, DynMoE）将大型 MLP 直接作为专家的思路不同，Tiny-ONN 将模型的复杂性从“单个专家的深度”转移到了“海量专家间的协同”，对门控网络（慢系统）的路由精度和训练策略（快系统）的资本效率提出了更高的要求。

## 2. 最终架构参数 (V3 - 精确对齐)

| 超参数                  | 值             | 备注                                                         |
| :---------------------- | :------------- | :----------------------------------------------------------- |
| **总参数量**            | **~0.509 B**   | **与 Qwen3-0.6B (0.508B) 对齐，因门控网络略增**         |
| `vocab_size`            | 151,936        | 继承自 Qwen3                                                 |
| `hidden_size`           | 1024           | 继承自 Qwen3                                                 |
| `num_hidden_layers`     | 28             | 继承自 Qwen3                                                 |
| `num_attention_heads`   | 16             | 继承自 Qwen3                                                 |
| `num_key_value_heads`   | 8              | 继承自 Qwen3 (GQA)                                           |
| `num_experts_per_layer` | **32**         | 核心设计：每层 MoE 总参数与原 MLP 层等价                     |
| `moe_intermediate_size` | **96**         | 核心设计：专家网络 FFN 维度 (`gate_proj`, `up_proj`)         |

**参数分布 (Tiny-ONN):**

- **共享骨架 (Shared Backbone)**: **243.66 M** (Embedding + All Attention Layers)
- **专家与门控 (Experts + Gating)**: **265.16 M** (总计 `28 * 32 = 896` 个专家及门控)
- **单个专家大小**: **0.295 M**

## 3. 训练策略：在线蒸馏与极致优化

### 3.1 核心范式：在线知识蒸馏

1. **骨架移植与冻结**: 加载预训练的 `Qwen3-0.6B` 模型，将其 **词嵌入** 和 **注意力层** 的权重完整复制到 Tiny-ONN 的共享骨架中，并**冻结**这些参数。
2. **专家层从零训练**: 随机初始化所有 896 个轻量级专家及门控网络。
3. **在线蒸馏流程**:
   - **同卡部署**: 在训练时，将**教师模型** (`Qwen3-0.6B`) 以 NF4 等 4-bit 量化方式加载。
   - **即时指导**: 在每个训练步骤中，输入数据同时通过两个模型，获得 `teacher_logits` 和 `student_logits`。
   - **损失计算**: 使用交叉熵损失函数计算 `student_logits` 与 `teacher_logits` (作为软标签) 之间的差距，仅对学生模型的可训练部分进行参数更新。
4. **SMK (Surprise Min_K) 策略**: 由于我们从 DynMoE 获得的全新经验，我们不再需要进行全面反向传播并取 min_k 进行更新，现在的 SMK 将直接进行类似标准 MoE 的选择性 Top_k 更新，但是路由网络的元学习任务被修正为根据路由的逐 token/patch 梯度 L2 范数进行交叉熵损失，从而学会指向 Surprise 最小的专家。

### 3.2 算力节约分析

我们的架构通过两种方式实现了极致的算力优化：

1. **继承预训练知识**: 我们保留了 Qwen3 预训练好的 `embedding` 和 `attention` 层。这意味着模型无需从零学习语言的底层规律，极大地降低了训练难度和数据需求。我们的任务被简化为：**在强大的预训练表征基础上，高效地训练新的 MoE 计算层。**

2. **稀疏激活降本**: 通过将稠密的 MLP 层替换为稀疏的 MoE 层，我们显著降低了前向传播的计算成本。
   - **原始 Dense MLP**: 在单层中处理一个 token 约需 **6.29M FLOPs** (`2 * hidden_size * intermediate_size`)。
   - **Tiny-ONN MoE (k=16)**: 约需 **3.15M FLOPs** (`16 * 2 * hidden_size * moe_intermediate_size`)。
   - **结论**: 在 FFN 计算环节，我们实现了约 **49.92%** 的计算量削减，这直接转化为更快的训练和推理速度。

### 3.3 硬件可行性

- **核心技术栈**:
  - **教师模型量化**: 使用 `bitsandbytes` 以 NF4 4-bit 格式加载。
  - **优化器**: 采用 8-bit AdamW 或 Adafactor 等显存优化型优化器。
  - **梯度检查点 (Gradient Checkpointing)**: **必须启用**，以应对 32K 长序列训练的激活值显存开销。
- **可行配置**: `batch_size=1`, `sequence_length=32,768`。
- **预估训练时间**: 基于 **0.5B tokens** 蒸馏数据，预估训练时间约为 **~1 天**。

## 4. 核心挑战与研究假设

项目的核心是验证一个科学假设：

> 一个设计精良的慢系统（动态门控 + 解耦元学习）能否有效指挥一个由海量“微功能”专家组成的快系统，以克服**表征瓶颈**和**路由精度**两大挑战，从而在极低的算力预算下，实现与同等参数规模的密集模型相媲美的性能。
