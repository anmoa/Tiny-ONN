# 开发日志：剪枝机制的重新思考与最终方案

## 2025-07-11: “乱码输出”的谜团

### 问题陈述

在重构模型剪枝逻辑后，剪枝后的模型虽然可以被正确加载，但在推理时会产生乱码。此问题在之前的 commit (`8309a66`) 中并不存在，尽管剪枝策略（剪掉哪些层）完全相同。这表明问题出在新的实现逻辑中，而非剪枝策略本身。

### 探索路径

我们探索了两种主要的剪枝方法，每种方法都有其自身的权衡和导致的问题。

#### 方案一：物理模块替换（“能用但有缺陷”的过去）

这是在最后一个已知良好 commit (`8309a66`) 中使用的方法。

- **实现**: 在 `PrunedQwen3DecoderLayer.__init__` 方法中，目标剪枝的模块（如 `self_attn`, `mlp`）被物理地替换为 `torch.nn.Identity()` 实例。
- **优点**:
  - 它能正常工作，推理能产生连贯的文本。
  - 被剪枝的模块没有参数，使得模型对象本身更轻量。
- **缺点**:
  - **致命缺陷**: 这种方法破坏了模型的物理结构完整性。`transformers` 库中的高级工具，如 `device_map="auto"`、`load_in_8bit` 和 `load_in_4bit`，在工作时会检查模型架构，并期望找到特定的模块（如 `Qwen3Attention`）。当它们找到 `nn.Identity()` 时，就会因为缺少预期的属性和方法而失败。这严重限制了模型的可用性。

#### 方案二：功能性剪枝（“理论正确但实际失败”的现在）

这是我们为了修复方案一的兼容性问题而进行的尝试。

- **实现**: 我们保持了模型完整的物理架构。`PrunedQwen3DecoderLayer` 总是实例化真正的 `Qwen3Attention` 和 `Qwen3MLP` 模块。我们引入布尔标志（`self.prune_self_attn`, `self.prune_mlp`）来控制 `forward` 方法中的数据流。如果一个标志为 `True`，相应模块的计算就会被跳过。
- **优点**:
  - 模型的结构与 `transformers` 库完全兼容，理论上解决了 `device_map` 和量化的问题。
- **缺点**:
  - 它会产生乱码输出。

### 对方案二失败的根本原因分析

经过排查，我们定位了可能的罪魁祸首。当 `AutoModelForCausalLM.from_pretrained` 加载我们的 `PrunedQwen3ForCausalLM` 模型时，它看到了完整的架构。然后，当它加载被我们剪枝过的 `state_dict` 时，它发现权重字典中的键是缺失的。

`transformers` 库正确地处理了这种情况，没有抛出错误。然而，它将那些缺失权重的模块**保留在了它们的随机初始化状态**。

因此，尽管我们的 `forward` 方法正确地**跳过了**对一个被剪枝模块的**计算**，但那个**带有随机权重**的模块本身仍然存在于模型对象和 PyTorch 的计算图中。这些巨大的、随机的张量，即使没有直接参与前向计算，也似乎在 `transformers` 运行机制的某个环节产生了未知的副作用，导致了最终的输出损坏。

### 前进之路：融合两种方案的优点

我们需要一个两全其美的解决方案：既有方案一的**功能正确性**，又有方案二的**结构完整性**。

新的设计原则是：**剪枝应该是被保存权重的属性，而不是模型架构本身的属性。**

这引出了一个清晰的、三步走的工作流程：

1. **模型架构 (`models/pruned_layers.py`)**: 模型代码将定义一个**逻辑上**可剪枝，但**物理上**完整的架构。`PrunedQwen3DecoderLayer` 将包含布尔标志（`prune_self_attn`, `prune_mlp`），其 `forward` 方法将使用这些标志来跳过计算。这确保了内存中的模型对象始终是结构健全且与 `transformers` 兼容的。

2. **剪枝脚本 (`scripts/PI_Pruning.py`)**: 此脚本的唯一职责是**对 `state_dict` 进行剪枝**。它将：
    a. 将完整的、预训练好的模型加载到我们的 `PrunedQwen3ForCausalLM` 架构中。
    b. 遍历所有层，根据 `prune_self_attn` 和 `prune_mlp` 标志，识别出 `state_dict` 中所有属于被剪枝模块的键。
    c. 创建一个**新的** `state_dict`，其中不包含被识别出的键。
    d. 将这个“瘦身”后的 `state_dict` 保存为 `model.safetensors` 文件。同时保存指向我们自定义架构的 `config.json`。

3. **加载脚本 (`scripts/fMRI.py`)**: 推理或分析脚本将：
    a. 使用 `AutoModelForCausalLM.from_pretrained(..., trust_remote_code=True)` 来实例化一个完整的模型骨架。这将根据 `config.json` 创建我们自定义的 `PrunedQwen3ForCausalLM`，其权重是随机初始化的。
    b. **关键步骤**：`from_pretrained` 方法随后会加载我们“瘦身”过的 `state_dict`。由于 `state_dict` 中的键不完整（这是我们有意为之的），`transformers` 会自动以 `strict=False` 的模式运行。它会加载所有存在的权重，而被剪枝的模块则会保留它们的随机初始化状态。

这个新方法是优越的，因为它遵循了 `transformers` 生态系统中的标准实践来处理部分权重加载。这是一个健壮、正确的处理我们需求的方案。我们将基于这份设计文档，从头开始重构这三个核心文件。
