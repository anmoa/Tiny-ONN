好的，博士。遵照您的指示，我将整合 `phase_2_architecture_and_training_plan.md` 和 `phase_2.1_implementation_plan.md` 的内容，生成一份统一的、全长的、作为最终蓝图的 **Phase-2 技术方案**。

这份文档将包含从战略洞察到具体编码实现的完整链路，确保我们下一步的行动有清晰、严谨的指引。

---

# Tiny-ONN Phase-2: 全长技术方案与实施蓝图

**文档版本:** 2.0
**日期:** 2025-07-13
**状态:** 最终确认

## 1. 战略背景与核心洞察

在最初的规划中，Phase-1 的目标是利用我们开发的“数字 fMRI”扫描仪，从一个预训练的稠密模型中提取“概念细胞”，然后在 Phase-2 中用这些细胞组装和训练 Tiny-ONN。

然而，在实践中我们获得了两个关键洞察：

1. **工具角色的混淆：** fMRI 扫描仪是一个完美的**“分析工具”**，适合精细观察，但它不是一个高效的**“生产工具”**，不适合用来大规模地提取专家。
2. **战略机遇的出现：** 我们可以升级扫描仪的用途，将其从一个“提取工具”转变为一个**“路由训练的监督信号生成器”**。

基于此，我们决定将 Phase-1 和 Phase-2 融合，形成一套全新的、更高效、更智能的训练范式：**我们不再试图提取专家，而是直接将稠密模型分解为专家，然后利用神经显影（`∫SPS`）来训练一个全新的路由系统去驾驭它们。**

## 2. Tiny-ONN 架构方案

### 2.1 核心设计原则

- **无损初始化 (Lossless Initialization):** 初始状态的 Tiny-ONN，在强制全激活模式下，其计算结果必须与原始的 Qwen3 模型在数学上完全等价。这是保证知识平稳迁移和后续训练稳定性的基石。
- **分阶段解耦训练 (Decoupled Training):**
    1. **路由学习阶段:** 冻结所有专家模块的权重，仅使用 `∫SPS` 信号作为监督，训练一个全新的、独立的路由系统。
    2. **专家分化阶段:** 在路由系统收敛后，解冻专家模块，并应用 Surprise Min-K (SMK) 策略进行持续的、稀疏的微调，以促进其功能分化。

### 2.2 分层高斯路由 (Hierarchical Gaussian Routing)

为实现物理决策深度（PDD）为 2 的内在推理能力，我们设计了一个分层的、类似决策树的路由系统。

- **两级结构:**
  - **1 x 主路由器:** 负责将任务路由到 10 个**专家组**之一。
  - **10 x 子路由器:** 每个专家组内含一个子路由器，负责从组内的 10 个专家中进行细粒度的 `top_k` 选择。
- **高斯路由:**
  - 每个专家（或专家组）的“专长领域”由一个可学习的**高斯分布 (`μ`, `σ²`)** 来定义。
  - 路由器的权重由输入与各个高斯分布的**概率密度**决定，实现了平滑且可解释的路由决策。

### 2.3 自注意力稀疏化的未来展望

在当前 MVP 阶段，我们将只对 MLP 模块进行稀疏化，以降低初期风险。在 MVP 成功后，我们将探索对自注意力层的稀疏化。

- **优先方向：激活稀疏化 (Sparse Attention):** 在计算注意力矩阵时，只计算部分 `(query, key)` 对的分数。此方案对模型稳定性冲击小，且有成熟算法可借鉴。
- **备选方向：权重稀疏化 (Mixture-of-Attentions, MoA):** 将整个自注意力模块也专家化。此方案实现复杂，对 KV 缓存管理和模型稳定性都带来了巨大挑战，将作为长期探索方向。

## 3. 训练范式：SPS 引导的自适应 SFT

这是我们训练范式的核心创新，它将神经显影与监督学习完美结合。

1. **SPS 扫描与监督信号生成:** 在路由学习阶段，通过一次完整的“伪稠密”前向传播和反向传播，为每个被激活的专家计算其 `SPS` 分数。对整个数据集的 `SPS` 进行积分，得到 `∫SPS`，并以此确定每个任务的“最优计算路径”。
2. **路由学习 (MNIST 难度):** 将“最优计算路径”作为监督学习的标签，训练路由系统。这个过程在数学上等价于一个简单的分类任务，从而避免了灾难性遗忘。
3. **SMK 专家分化:** 在专家学习阶段，应用 Surprise Min-K (SMK) 策略，仅更新梯度范数（`Surprise`）最小的 `min_k` 个专家的权重，从而鼓励知识的稳定深化，并允许采用更高的、永不调度的固定学习率。

## 4. 编码实施蓝图

基于对 `transformers` 库中 Qwen3 模块化源码（`modular_qwen3.py`）的分析，我们制定了清晰的、分文件的编码实施计划。

### 4.1 `tinyonn/moe.py`: 混合专家核心模块

此文件将定义构成我们稀疏化 Transformer 层的核心组件。

- **`TinyONNExpert(nn.Module)`:**
  - **实现:** 直接继承或包装 `Qwen2MLP`。`Qwen2MLP` 是一个标准的 SwiGLU 实现，结构清晰，便于我们后续的权重复制和操作。
- **`GaussianMoERouter(nn.Module)`:**
  - **实现:** 包含一个 `nn.Linear` 层用于生成 `logits`，以及存储专家（或专家组）高斯分布 `μ` 和 `σ²` 的参数。其 `forward` 方法将实现基于概率密度的路由权重计算。
- **`MoELayer(nn.Module)` (采用“软专家”策略):**
  - **实现:** 包含一个 `GaussianMoERouter` 和一个 `nn.ModuleList` 的 `TinyONNExpert`。
  - **核心逻辑:** 实现“软专家”策略。其 `forward` 方法将调用路由器获得权重，然后对专家的**权重本身**进行加权求和，动态合成一个临时的“巨专家”(Mega-Expert)，并用其处理输入。
  - **优势:**
    - **计算高效:** 将 `k` 次矩阵乘法优化为 `1` 次。
    - **无损保证:** 在所有专家权重相同的初始状态下，合成的权重与原始权重完全等价，完美实现了无损初始化。

### 4.2 `tinyonn/layer.py`: Tiny-ONN Transformer 层

此文件定义了 Tiny-ONN 的基本构建单元。

- **`TinyONNLayer(Qwen2DecoderLayer)`:**
  - **继承:** 继承自 `Qwen2DecoderLayer`，以复用其标准 Transformer 层结构。
  - **改造:**
    - **`self_attn`:** **保持 `Qwen3Attention` 不变**，以确保稳定性和兼容性。
    - **`mlp`:** 用我们自己实现的 `tinyonn.moe.MoELayer` 替换掉原始的 `mlp` 模块。

### 4.3 `tinyonn/model.py`: 顶层模型定义

此文件将组装最终的 Tiny-ONN 模型。

- **`TinyONNModel(Qwen2Model)`:**
  - **继承:** 继承自 `Qwen2Model`。
  - **改造:** 其 `layers` 属性将是一个 `nn.ModuleList` of `tinyonn.layer.TinyONNLayer`。
- **`TinyONNForCausalLM(Qwen2ForCausalLM)`:**
  - **继承:** 继承自 `Qwen2ForCausalLM`，以复用其完整的 Causal LM 功能和 `generate` 方法接口。
  - **改造:** 其内部的 `model` 属性将是我们定义的 `TinyONNModel`。

## 5. 对模型潜力的再评估

- **总参数量:** 约 **1.72B**，仅因增加了路由系统而略微增大。
- **计算量 (FLOPs):** 可通过我们设计的**无参数自适应门控 (PFAG)** 机制，根据任务的瞬时难度进行动态调控，实现代差级别的效率提升。
- **可扩展性:** 架构天然支持**专家扩容 (Expert Expansion)**，允许通过增加新专家来持续提升模型的知识容量和能力上限，使其成为一个具备长期演化潜力的系统。
