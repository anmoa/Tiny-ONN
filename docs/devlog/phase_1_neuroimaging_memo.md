# Phase-1 神经显影

**版本:** 1.2
**日期:** 2025-07-12

## 1. 目标

本文档旨在为 Tiny-ONN 项目的“Phase-1”设计并规划一个自动化、可扩展且高效的批量扫描工作流。该工作流的核心目标是，能够针对大规模、经过特定主题裁剪的数据集，系统性地执行 fMRI 扫描（即捕获模型内部的激活与梯度信息），并生成结构统一、易于分析的 `.mscan` 数据文件。

## 2. 核心原则

- **关注点分离 (Separation of Concerns)**: 批量数据处理的逻辑应与交互式探索的 UI 完全分离。为此，我们将创建一个独立的、面向命令行的批量扫描执行器，与现有的 `scanner/app.py` 解耦。
- **数据驱动的特异性 (Data-Driven Specificity)**: 我们将通过创建专门化的、经过主题裁剪的数据集来实现内容类别的专门化分析。例如，我们会生成 `python_corpus.jsonl` 或 `finance_corpus.jsonl` 这样的数据集。这种方法避免了在 `.mscan` 元数据中增加不必要的复杂性，使得每个 `.mscan` 文件天然地对应一个特定主题。
- **并行开发与安全替换 (Parallel Development & Safe Replacement)**: 新的核心功能（如统一扫描引擎）将在不影响现有代码的情况下并行开发。只有在新功能被充分验证后，才会通过修改 `__init__.py` 等方式替换旧的实现，并最终在确认稳定后移除旧代码。

## 3. 工作流详解

整个工作流分为三个主要阶段：

### 阶段一：数据集裁剪

此阶段的目标是生成用于专门化分析的语料库。

- **任务**: 编写一个或多个预处理脚本。
- **位置**: `scripts/preprocessing/`
- **功能**:
  - 从原始的大规模 SFT 数据集（如 `HuggingFaceH4/ultrachat_200k`）中，根据关键词、代码块或其他启发式规则，筛选和提取特定主题的样本。
  - 将筛选出的样本保存为新的、独立的 `.jsonl` 文件。
- **输出**: 一系列主题明确的数据集文件，例如：
  - `data/sft/python_corpus.jsonl`
  - `data/sft/finance_corpus.jsonl`

### 阶段二：批量扫描执行器

此阶段是整个自动化流程的核心，负责调度和执行扫描任务。

- **任务**: 创建一个新的主执行脚本 `scripts/batch_scanner.py`。
- **功能**:
  1. **参数解析**: 脚本通过命令行接收输入：
     - `--model_path`: 要扫描的模型的路径。
     - `--dataset_path`: 目标数据集文件的路径。
     - `--output_path`: 输出的 `.mscan` 文件的完整路径。
  2. **初始化**:
     - 加载指定的模型和分词器。
     - 调用 `scanner.io.create_mscan_file` 创建一个空的 `.mscan` 文件。
  3. **迭代与扫描**:
     - 逐一读取数据集中的每个样本。
     - 对每个样本，调用 `scanner.engine` 中的扫描函数。
  4. **实时存储**:
     - 将返回的扫描数据和元信息传递给 `scanner.io.append_records_to_mscan`，实时追加到 `.mscan` 文件中。

### 阶段三：统一扫描引擎探索 [已废弃]

此阶段的目标是探索一个代码更内聚、逻辑更统一的扫描引擎。

- **探索方向**: 创建一个单一的 `run_unified_scan` 函数，在同一个循环中逐 token 完成生成、激活捕获和梯度捕获。
- **核心挑战**: 该方案要求在循环的每一步都调用 `loss.backward(retain_graph=True)`，以保留用于后续 token 生成的计算图。
- **最终结论: 此路不通**。经过深入的算法复杂度分析和性能测试，我们得出最终结论：
  - **性能灾难**: `retain_graph=True` 方案导致了 `O(N^2)`（N为生成token数）的二次方复杂度。每生成一个新 token，由于需要维护和回溯的计算图不断增长，梯度计算的耗时会急剧增加，最终导致性能完全不可接受。
  - **理论局限的规避**: 尽管逐 token 计算损失的模式在单次测量上是“短视”的（无法捕捉 token 对未来的贡献），但本项目的核心评估指标 `∫SPS` 是一个宏观统计量。通过对海量 token 的 SPS 分数（高激活 vs 低梯度）进行积分，我们能够有效地识别出在各种上下文中持续表现出功能稳定性和重要性的参数，从而在统计层面规避了单次测量的理论局限。
- **决策**: 鉴于统一引擎存在根本性的性能缺陷，且其在梯度计算的理论层面并未带来超越旧引擎的优势，**本项目决定正式废弃此探索方向**。我们将回归并继续依赖经过验证的、性能可预测的分离式扫描引擎。

## 4. 最终文件结构

```
Tiny-ONN/
├── scanner/
│   ├── engine/
│   │   ├── __init__.py
│   │   ├── forward_pass.py       # (核心扫描逻辑)
│   │   └── backward_pass.py      # (核心扫描逻辑)
│   └── io.py
├── scripts/
│   ├── batch_scanner.py          # (阶段二的核心)
│   └── preprocessing/
│       └── create_corpus.py      # (阶段一的示例)
└── data/
    ├── sft/
    │   └── python_corpus.jsonl   # (阶段一的输出)
    └── scans/
        └── Qwen3-1.7B_python.mscan # (阶段二的输出)
