# 技术备忘录：梯度捕获方案选型

## 1. 问题陈述

在我们的 `1f2b2o` 训练范式中，一个核心技术挑战是：如何在不显著增加计算或显存开销的前提下，精确地捕获 `per-token-per-expert` 粒度的中间激活值及其对应的梯度，以用于门控网络的元学习。

我们需要一个机制，该机制能在主损失 (`main_loss`) 的反向传播过程中“窃取”我们感兴趣的张量，并将它们安全地传递给后续的门控损失 (`gating_loss`) 计算，同时允许主计算图在第一次优化后被安全释放。

## 2. 方案比较

我们评估了三种主流的技术方案：

### 方案 A: Backward Hooks (`register_full_backward_hook`)

- **原理**: 在模块或张量上注册一个钩子函数，该函数会在其梯度被计算时触发，从而可以访问到梯度值。
- **优点**: 灵活，是 PyTorch 提供的标准调试和扩展工具。
- **缺点**:
  - **信息不全**: 钩子函数通常只能访问到梯度 (`grad_input`, `grad_output`)，而难以同时获取其对应的、在前向传播中产生的激活值。
  - **实现复杂**: 需要仔细管理钩子的注册和移除，容易在复杂的模型和训练循环中出错。
  - **可读性差**: 将核心的计算逻辑分散到回调函数中，降低了代码的可读性和可维护性。

### 方案 B: `retain_graph=True`

- **原理**: 在第一次 `backward()` 时设置 `retain_graph=True`，完整保留计算图，以便后续的梯度计算。这是我们在早期 PoC 中（如 `smk_poc.py` 中部分实现）采用的思路。
- **优点**: 实现极其简单，直观。
- **缺点**:
  - **显存灾难**: 保留整个计算图意味着所有中间激活值都必须驻留在显存中，直到所有 `backward()` 调用结束。正如我们的[显存分析文档](memory_analysis.md)所证实的，这会导致显存占用呈数量级增长，对于稍大一些的模型或批次大小都完全不可行。

### 方案 C: `torch.autograd.Function` + 全局缓存 (最终方案)

- **原理**: 创建一个自定义的 `autograd` 算子，它在计算图中的作用是恒等映射，其唯一目的是利用其 `forward` 和 `backward` 方法的特性，将中间张量捕获到一个外部的全局缓存中。
- **优点**:
  - **精确捕获**: 可以在 `forward` 过程中捕获激活值，在 `backward` 过程中捕获其对应的梯度，实现了信息的完美配对。
  - **显存高效**: 捕获的张量都经过了 `.detach().clone()`，与主计算图完全分离。主计算图可以在第一次优化后被立即释放，实现了 `1f2b2o` 的核心要求。
  - **概念清晰**: 将捕获逻辑封装在一个独立的、可复用的 `CaptureActivation` 函数中，代码结构清晰，主训练循环逻辑干净。
  - **实现简单**: 使用一个简单的模块级全局字典作为缓存，在研究阶段避免了过度工程化，让我们能聚焦于核心算法的验证。

## 3. 最终决策

综合考虑理论严谨性、计算效率和工程可行性，我们最终选择**方案 C (`torch.autograd.Function` + 全局缓存)**作为本项目的标准梯度捕获方案。

该方案以一种优雅且高效的方式，完美解决了 `1f2b2o` 范式中的核心数据流问题，是我们后续所有理论和实验的基石。具体实现可参考我们更新后的核心架构文档。
